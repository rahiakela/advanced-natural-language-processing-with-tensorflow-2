{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-understanding-sentiment-using-glove-based-transfer-learning.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNNKjxCN5odssGtzScwCK+O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/advanced-natural-language-processing-with-tensorflow-2/blob/main/4-transfer-learning/1_understanding_sentiment_using_glove_based_transfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZQmMt3biyFJ"
      },
      "source": [
        "##Understanding Sentiment using GloVe based transfer learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2oEUmwSjwFy"
      },
      "source": [
        "We have used BiLSTM model to predict the sentiment of IMDb movie reviews. That model learned embeddings of the words from scratch. This model had an accuracy of `83.55%` on the test set, while the SOTA result was closer to `97.4%`. If pre-trained embeddings are used, we expect an increase in model accuracy. \n",
        "\n",
        "After all the setup is completed, we will need to use TensorFlow to use these pre-trained embeddings. There will be two different models that will be tried â€“ \n",
        "- the first will be based on feature extraction\n",
        "- the second one on fine-tuning\n",
        "\n",
        "Let's try this out and see the impact of transfer learning on this model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAlD10-fkC9C"
      },
      "source": [
        "##Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "u34tQYtIkEHv",
        "outputId": "38871871-7ee9-4a95-e46e-7446de02be0d"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.5.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hjVwYZ7kTr1",
        "outputId": "95595517-9d16-4186-f014-e51ca4be9fb9"
      },
      "source": [
        "######## GPU CONFIGS FOR RTX 2070 ###############\n",
        "## Please ignore if not training on GPU       ##\n",
        "## this is important for running CuDNN on GPU ##\n",
        "\n",
        "tf.keras.backend.clear_session() #- for easy reset of notebook state\n",
        "\n",
        "# chck if GPU can be seen by TF\n",
        "tf.config.list_physical_devices('GPU')\n",
        "# only if you want to see how commands are executed\n",
        "#tf.debugging.set_log_device_placement(True)\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  # Restrict TensorFlow to only use the first GPU\n",
        "  try:\n",
        "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
        "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
        "  except RuntimeError as e:\n",
        "    # Visible devices must be set before GPUs have been initialized\n",
        "    print(e)\n",
        "###############################################"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 Physical GPUs, 1 Logical GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LojXqIvwhSE",
        "outputId": "2ba3411f-1cfd-4d39-9b0b-c29c691b626d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Download the GloVe embeddings\n",
        "!wget -q http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpQRIAuqklY1"
      },
      "source": [
        "##Loading IMDb training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_KDedC2kmPN"
      },
      "source": [
        "TensorFlow Datasets or the tfds package will be used to load the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_KrBaGTkqro"
      },
      "source": [
        "imdb_train, ds_info = tfds.load(name=\"imdb_reviews\", split=\"train\", with_info=True, as_supervised=True)\n",
        "imdb_test = tfds.load(name=\"imdb_reviews\", split=\"test\", as_supervised=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUvRVMTVm8qc",
        "outputId": "7e9cdbc1-d4ce-403c-d564-fc50001f3380"
      },
      "source": [
        "# Check label and example from the dataset\n",
        "for example, label in imdb_train.take(1):\n",
        "  print(example, \"\\n\", label)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", shape=(), dtype=string) \n",
            " tf.Tensor(0, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmIXaviKnVri"
      },
      "source": [
        "## Create Vocab and Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2LLB3egnYBI"
      },
      "source": [
        "After the training and test sets are loaded, the content of the reviews needs to be tokenized and encoded:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJ_r73znnK50"
      },
      "source": [
        "# Use the default tokenizer settings\n",
        "tokenizer = tfds.deprecated.text.Tokenizer()\n",
        "\n",
        "vocabulary_set = set()\n",
        "MAX_TOKENS = 0\n",
        "\n",
        "for example, label in imdb_train:\n",
        "  some_tokens = tokenizer.tokenize(example.numpy())\n",
        "  if MAX_TOKENS < len(some_tokens):\n",
        "    MAX_TOKENS = len(some_tokens)\n",
        "  vocabulary_set.update(some_tokens)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1OrV8IVobgq"
      },
      "source": [
        "We tokenizes the review text and constructs a vocabulary.\n",
        "This vocabulary is used to construct a tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXZQEDpQodFR",
        "outputId": "b7e454c9-8038-48c1-8fea-ff6b227e001b"
      },
      "source": [
        "imdb_encoder = tfds.deprecated.text.TokenTextEncoder(vocabulary_set, lowercase=True, tokenizer=tokenizer)\n",
        "vocab_size = imdb_encoder.vocab_size\n",
        "\n",
        "print(vocab_size, MAX_TOKENS)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "93931 2525\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykT8a-7Fp_6L"
      },
      "source": [
        "Note that text was converted to lowercase before encoding. Converting to lowercase helps reduce the vocabulary size and may benefit the lookup of corresponding GloVe vectors. Note that capitalization may contain important information, which may help in tasks such as NER."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnH7YoWjpt05",
        "outputId": "42ac8ce2-60e3-4ac8-9c48-d566153961f6"
      },
      "source": [
        "# Lets verify tokenization and encoding works\n",
        "for example, label in imdb_train.take(1):\n",
        "  print(example, \"\\n\")\n",
        "  encoded = imdb_encoder.encode(example.numpy())\n",
        "  print(encoded, \"\\n\")\n",
        "  print(imdb_encoder.decode(encoded))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", shape=(), dtype=string) \n",
            "\n",
            "[57134, 72469, 82410, 88200, 51576, 84943, 62773, 57508, 89461, 89894, 83203, 87043, 58496, 58774, 81304, 85480, 15782, 73561, 68007, 85479, 45055, 74551, 57134, 72145, 84789, 89461, 86090, 83493, 85081, 83203, 84907, 45565, 86090, 85479, 91221, 39709, 58180, 47978, 57134, 84943, 89939, 49608, 57634, 57134, 84943, 92432, 82410, 88896, 83154, 82589, 89183, 40912, 77552, 60082, 71226, 84914, 56894, 34678, 67720, 77552, 39756, 37323, 56894, 86380, 86090, 70778, 88204, 82581, 88934, 72006, 64482, 77802, 45108, 78363, 56800, 85711, 60381, 83430, 40905, 58774, 72469, 88467, 74551, 81061, 71226, 63010, 45789, 83203, 81061, 84943, 92629, 72469, 62694, 72541, 62348, 78970, 93662, 18537, 69702, 71347, 92629, 67104, 68007, 81008, 67417, 57134, 86492, 67610, 89939, 67417, 58496, 58774, 89939, 27263, 48484, 18537, 39709, 90022, 54398, 87840, 67367] \n",
            "\n",
            "this was an absolutely terrible movie don t be lured in by christopher walken or michael ironside both are great actors but this must simply be their worst role in history even their great acting could not redeem this movie s ridiculous storyline this movie is an early nineties us propaganda piece the most pathetic scenes were those when the columbian rebels were making their cases for revolutions maria conchita alonso appeared phony and her pseudo love affair with walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning i am disappointed that there are movies like this ruining actor s like christopher walken s good name i could barely sit through it\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UneWjRGgtE-u"
      },
      "source": [
        "Now that the tokenizer is ready, the data needs to be tokenized, and sequences\n",
        "padded to a maximum length. Since we are interested in comparing performance\n",
        "with the previosly trained model,we can use the same setting of sampling a maximum of 150 words of the review."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JexztWE6rYW5"
      },
      "source": [
        "# transformation functions to be used with the dataset\n",
        "def encode_pad_transform(sample):\n",
        "  encoded = imdb_encoder.encode(sample.numpy())\n",
        "  pad = sequence.pad_sequences([encoded], padding=\"post\", maxlen=150)\n",
        "\n",
        "  return np.array(pad[0], dtype=np.int64)\n",
        "\n",
        "def encode_tf_fn(sample, label):\n",
        "  encoded = tf.py_function(encode_pad_transform, inp=[sample], Tout=(tf.int64))\n",
        "  encoded.set_shape([None])\n",
        "  label.set_shape([])\n",
        "\n",
        "  return encoded, label"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6P-20m9YuJWH"
      },
      "source": [
        "# test the transformation on a small subset\n",
        "subset = imdb_train.take(10)\n",
        "tst = subset.map(encode_tf_fn)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oj_T4RM2ufAT",
        "outputId": "833d3166-32f0-491d-e5b2-b556d6575d90"
      },
      "source": [
        "for review, label in tst.take(1):\n",
        "  print(review, label)\n",
        "  print(\"\\n\", imdb_encoder.decode(review))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[57134 72469 82410 88200 51576 84943 62773 57508 89461 89894 83203 87043\n",
            " 58496 58774 81304 85480 15782 73561 68007 85479 45055 74551 57134 72145\n",
            " 84789 89461 86090 83493 85081 83203 84907 45565 86090 85479 91221 39709\n",
            " 58180 47978 57134 84943 89939 49608 57634 57134 84943 92432 82410 88896\n",
            " 83154 82589 89183 40912 77552 60082 71226 84914 56894 34678 67720 77552\n",
            " 39756 37323 56894 86380 86090 70778 88204 82581 88934 72006 64482 77802\n",
            " 45108 78363 56800 85711 60381 83430 40905 58774 72469 88467 74551 81061\n",
            " 71226 63010 45789 83203 81061 84943 92629 72469 62694 72541 62348 78970\n",
            " 93662 18537 69702 71347 92629 67104 68007 81008 67417 57134 86492 67610\n",
            " 89939 67417 58496 58774 89939 27263 48484 18537 39709 90022 54398 87840\n",
            " 67367     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0], shape=(150,), dtype=int64) tf.Tensor(0, shape=(), dtype=int64)\n",
            "\n",
            " this was an absolutely terrible movie don t be lured in by christopher walken or michael ironside both are great actors but this must simply be their worst role in history even their great acting could not redeem this movie s ridiculous storyline this movie is an early nineties us propaganda piece the most pathetic scenes were those when the columbian rebels were making their cases for revolutions maria conchita alonso appeared phony and her pseudo love affair with walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning i am disappointed that there are movies like this ruining actor s like christopher walken s good name i could barely sit through it\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2f269CWu9JF"
      },
      "source": [
        "Finally, the data is encoded using the convenience functions above like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tv1MIsBxuyUG"
      },
      "source": [
        "# now tokenize/encode/pad all training and testing data\n",
        "encoded_train = imdb_train.map(encode_tf_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "encoded_test = imdb_test.map(encode_tf_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsG9s3dcvbcG"
      },
      "source": [
        "At this point, all the training and test data is ready for training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0zplq4IvcCE"
      },
      "source": [
        "## Loading pre-trained GloVe embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwCPpb_8wTWo"
      },
      "source": [
        "The next step is the foremost step in transfer learning â€“ loading the pre-trained GloVe embeddings and using these as the weights of the embedding layer.\n",
        "\n",
        "The nearest GloVe dimension is 50, so let's use that. The file format is quite simple. Each line of the text has multiple values separated by spaces. The first item of each row is the word, and the rest of the items are the values of the vector for each dimension. So, in the 50-dimensional file, each row will have 51 columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95QVg6XhvV-v"
      },
      "source": [
        "dict_w2v = {}\n",
        "\n",
        "with open(\"glove.6B.50d.txt\", \"r\") as file:\n",
        "  for line in file:\n",
        "    tokens = line.split()\n",
        "    word = tokens[0]\n",
        "    vector = np.array(tokens[1:], dtype=np.float32)\n",
        "\n",
        "    if vector.shape[0] == 50:\n",
        "      dict_w2v[word] = vector\n",
        "    else:\n",
        "      print(\"There was an issue with \", vector)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KC4_pws83Vh-"
      },
      "source": [
        "You should see a dictionary size of 400,000 words. Once these vectors are loaded, an embedding matrix needs to be created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdFsiC232-0u",
        "outputId": "28911078-586b-4257-fe57-e6883a8e6053",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# let's check the vocabulary size\n",
        "print(\"Dictionary Size: \", len(dict_w2v))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dictionary Size:  400000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFRifDVx3c_h"
      },
      "source": [
        "##Creating a pre-trained embedding matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyDXc-Xm3drA"
      },
      "source": [
        "So far, we have a dataset, its vocabulary, and a dictionary of GloVe words and\n",
        "their corresponding vectors. However, there is no correlation between these two\n",
        "vocabularies. The way to connect them is through the creation of an embedding\n",
        "matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YR0kuomt3GRl"
      },
      "source": [
        "# First, let's initialize an embedding matrix of zeros\n",
        "embedding_dim = 50\n",
        "embedding_matrix = np.zeros((imdb_encoder.vocab_size, embedding_dim))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilWEy8494LkO"
      },
      "source": [
        "Note that this is a crucial step. When a pre-trained word list is used, finding a vector for each word in the training/test is not guaranteed.\n",
        "\n",
        "After this embedding matrix of zeros is initialized, it needs to be populated. For each word in the vocabulary of reviews, the corresponding vector is retrieved from the GloVe dictionary.\n",
        "\n",
        "The ID of the word is retrieved using the encoder, and then the embedding matrix\n",
        "entry corresponding to that entry is set to the retrieved vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9n0W_Ey4BX_"
      },
      "source": [
        "unk_cnt = 0\n",
        "unk_set = set()\n",
        "\n",
        "for word in imdb_encoder.tokens:\n",
        "  embedding_vector = dict_w2v.get(word)\n",
        "\n",
        "  if embedding_vector is not None:\n",
        "    token_id = imdb_encoder.encode(word)[0]\n",
        "    embedding_matrix[token_id] = embedding_vector\n",
        "  else:\n",
        "    unk_cnt += 1\n",
        "    unk_set.add(word)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mQd9ddy5_Qa",
        "outputId": "322116e8-a634-486d-8e27-f8ee240aa937",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# how many weren't found?\n",
        "print(\"Total unknown words: \", unk_cnt)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total unknown words:  14553\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAbOr4XT7hoi"
      },
      "source": [
        "During the data loading step, we saw that the total number of tokens was 93,931.\n",
        "Out of these, 14,553 words could not be found, which is approximately 15% of\n",
        "the tokens. For these words, the embedding matrix will have zeros.\n",
        "\n",
        "**This is the first step in transfer learning.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8JDtcBI-AKs"
      },
      "source": [
        "##Feature extraction model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47byQiBj-A-u"
      },
      "source": [
        "The feature extraction model freezes the pre-trained\n",
        "weights and does not update them. An important issue with this approach in the\n",
        "current setup is that there are a large number of tokens, over 14,000, that have\n",
        "zero embedding vectors. These words could not be matched to an entry in the\n",
        "GloVe word list.\n",
        "\n",
        "---\n",
        "To minimize the chances of not finding matches between the\n",
        "pre-trained vocabulary and task-specific vocabulary, ensure\n",
        "that similar tokenization schemes are used.\n",
        "\n",
        "GloVe uses a wordbased tokenization scheme like the one provided by the Stanford\n",
        "tokenizer.This works better than a whitespace tokenizer.\n",
        "\n",
        "We see 15% unmatched tokens due to different tokenizers.\n",
        "\n",
        "As an exercise, we can implement the Stanford tokenizer\n",
        "and see the reduction in unknown tokens.\n",
        "\n",
        "Newer methods like BERT use parts of subword tokenizers.\n",
        "Subword tokenization schemes can break up words into parts,\n",
        "which minimizes this chance of mismatch in tokens. Some\n",
        "examples of subword tokenization schemes are Byte Pair Encoding\n",
        "(BPE) or WordPiece tokenization.\n",
        "\n",
        "---\n",
        "\n",
        "If pre-trained vectors were not used, then the vectors for all the words would start with nearly zero and get trained through gradient descent. \n",
        "\n",
        "In this case, the vectors are already trained, so we expect the training to go along much faster. \n",
        "\n",
        "For a baseline, one epoch of training of the BiLSTM model while training embeddings takes between 65 seconds and 100 seconds.\n",
        "\n",
        "Now, let's build the model and plug in the embedding matrix generated above into\n",
        "the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mSJBhgQBJSe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKkgUOl4-CRq"
      },
      "source": [
        "##Fine-tuning model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8jZ8vH6-KGj"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4HL-BM_6FYI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}