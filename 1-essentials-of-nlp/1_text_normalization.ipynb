{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "1-text-normalization.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "RS6j_Uu1aKr7",
        "Q5NI7lL_aPrR",
        "50W5rWfeOoR3"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/advanced-natural-language-processing-with-tensorflow-2/blob/main/1-essentials-of-nlp/1_text_normalization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdA7sBdGl2S2"
      },
      "source": [
        "# SMS Spam Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Xk0ZWFhsJg7"
      },
      "source": [
        "To understand how to process text, it is important to understand the general\r\n",
        "workflow for NLP.\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/advanced-nlp-with-tensorflow-2/text-processing-workflow.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "The first two steps of the process in the preceding diagram involve collecting labeled data. A supervised model or even a semi-supervised model needs data to operate.\r\n",
        "\r\n",
        "The next step is usually normalizing and featurizing the data. Models have a hard time processing text data as is. There is a lot of hidden structure in a given text that needs to be processed and exposed. These two steps focus on that. \r\n",
        "\r\n",
        "The last step is building a model with the processed inputs. While NLP has some unique models, this chapter will use only a simple deep neural network and focus more on the normalization and vectorization/featurization. Often, the last three stages operate in a cycle, even though the diagram may give the impression of linearity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZPInUZktiyI"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9xCqLf9f0hM"
      },
      "source": [
        "%%shell\r\n",
        "\r\n",
        "pip install stopwordsiso\r\n",
        "pip install stanfordnlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zy9eSdb1cn4Z"
      },
      "source": [
        "%tensorflow_version 2.x     # magic command instructing to use TensorFlow version 2+\n",
        "import tensorflow as tf\n",
        "#from tf.keras.models import Sequential\n",
        "#from tf.keras.layers import Dense\n",
        "import os\n",
        "import io\n",
        "import re\n",
        "\n",
        "import pandas as pd \n",
        "import stopwordsiso as stopwords\n",
        "import stanfordnlp as snlp\n",
        "en = snlp.download('en')\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ3vRSqLvTu6"
      },
      "source": [
        "# Data collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wr1vzM27oyLD"
      },
      "source": [
        "**The first step of any Machine Learning (ML) project is to obtain a dataset.**\r\n",
        "\r\n",
        "We will be using the SMS Spam Collection dataset made available by University of California, Irvine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JSjikP_c0Ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69c8ec8b-3755-43a2-e2c9-416b437f8589"
      },
      "source": [
        "# Download the zip file\n",
        "path_to_zip = tf.keras.utils.get_file(\"smsspamcollection.zip\",\n",
        "                  origin=\"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\",\n",
        "                  extract=True)\n",
        "\n",
        "# Unzip the file into a folder\n",
        "!unzip $path_to_zip -d data"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
            "204800/203415 [==============================] - 0s 1us/step\n",
            "Archive:  /root/.keras/datasets/smsspamcollection.zip\n",
            "  inflating: data/SMSSpamCollection  \n",
            "  inflating: data/readme             \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytFnxCGFaJ20"
      },
      "source": [
        "# optional step - helps if colab gets disconnected\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbXeir-dm0nr"
      },
      "source": [
        "Reading the data file is trivial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Kb8AF3wHaKE3",
        "outputId": "2f92f1cb-3da8-493f-944b-442cc2bd7f40"
      },
      "source": [
        "# Let's see if we read the data correctly\n",
        "# lines = io.open('/content/drive/My Drive/colab-data/SMSSpamCollection').read().strip().split('\\n')\n",
        "lines = io.open('/content/data/SMSSpamCollection').read().strip().split('\\n')\n",
        "lines[0]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ham\\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "dYG5LFyDm-C-",
        "outputId": "7196635c-5f31-47f8-e4b3-ba63505b1edf"
      },
      "source": [
        "lines[2]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"spam\\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCFBMGdWvnNn"
      },
      "source": [
        "## Pre-process Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zle10-0pnUjv"
      },
      "source": [
        "The next step is to split each line into two columns â€“ one with the text of the message and the other as the label. While we are separating these labels, we will also convert the labels to numeric values. Since we are interested in predicting spam messages, we can assign a value of 1 to the spam\r\n",
        "messages. A value of 0 will be assigned to legitimate messages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhpfi9lWC5We",
        "outputId": "b60ad0b9-56e7-4758-956c-96724bc23488"
      },
      "source": [
        "spam_dataset = []\n",
        "spam_count = 0\n",
        "ham_count = 0\n",
        "for line in lines:\n",
        "  label, text = line.split('\\t')\n",
        "  if label.lower().strip() == 'spam':\n",
        "    spam_dataset.append((1, text.strip()))\n",
        "    spam_count += 1\n",
        "  else:\n",
        "    spam_dataset.append(((0, text.strip())))\n",
        "    ham_count += 1\n",
        "\n",
        "spam_dataset[:5]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'),\n",
              " (0, 'Ok lar... Joking wif u oni...'),\n",
              " (1,\n",
              "  \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\"),\n",
              " (0, 'U dun say so early hor... U c already then say...'),\n",
              " (0, \"Nah I don't think he goes to usf, he lives around here though\")]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aV1uWIEUoLWQ",
        "outputId": "3f64a4be-0a17-4042-ad42-02d055d09987"
      },
      "source": [
        "print(\"Spam: \", spam_count, \", Ham: \", ham_count)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spam:  747 , Ham:  4827\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JReDkDrhoSWG"
      },
      "source": [
        "Now the dataset is ready for further processing in the pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SkoqwjizbBS"
      },
      "source": [
        "# Data Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hfv0a0JegCr"
      },
      "source": [
        "Text normalization is a pre-processing step aimed at improving the quality\r\n",
        "of the text and making it suitable for machines to process. \r\n",
        "\r\n",
        "Four main steps in text normalization are:\r\n",
        "\r\n",
        "- case normalization, \r\n",
        "- tokenization and stop word removal,\r\n",
        "- Parts-of-Speech (POS) tagging, \r\n",
        "- and stemming."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuqsxAIAmT4l"
      },
      "source": [
        "## Case normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkX0qQ_Xmrx_"
      },
      "source": [
        "**Case normalization applies to languages that use uppercase and lowercase letters.**\r\n",
        "\r\n",
        "All languages based on the Latin alphabet or the Cyrillic alphabet (Russian,\r\n",
        "Mongolian, and so on) use upper- and lowercase letters. Other languages\r\n",
        "that sometimes use this are Greek, Armenian, Cherokee, and Coptic. \r\n",
        "\r\n",
        "In case normalization, all letters are converted to the same case. It is quite helpful in semantic use cases. However, in other cases, this may hinder performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "js-XjZJmltdp"
      },
      "source": [
        "### Preprocessing case normalized data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i3JudyHl0bs"
      },
      "source": [
        "Let's build a baseline model with three simple features:\r\n",
        "\r\n",
        "- Number of characters in the message\r\n",
        "- Number of capital letters in the message\r\n",
        "- Number of punctuation symbols in the message"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFCghF5YLcmb"
      },
      "source": [
        "# To do so, first, we will convert the data into a pandas DataFrame\r\n",
        "df = pd.DataFrame(spam_dataset, columns=['Spam', 'Message'])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "DT0spdItiWIo",
        "outputId": "cfc36600-7b6d-4526-e346-c7835ad43142"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Spam</th>\n",
              "      <th>Message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Spam                                            Message\n",
              "0     0  Go until jurong point, crazy.. Available only ...\n",
              "1     0                      Ok lar... Joking wif u oni...\n",
              "2     1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3     0  U dun say so early hor... U c already then say...\n",
              "4     0  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8O615kHKggOz"
      },
      "source": [
        "Next, let's build some simple functions that can count the length of the message, and the numbers of capital letters and punctuation symbols. Python's regular expression package, re, will be used to implement these:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRQZpvvHhI9K"
      },
      "source": [
        "# Normalization functions\n",
        "\n",
        "def message_length(x):\n",
        "  # returns total number of characters\n",
        "  return len(x)\n",
        "\n",
        "def num_capitals(x):\n",
        "  # get count of capital letters\n",
        "  _, count = re.subn(r'[A-Z]', '', x) # only works in english\n",
        "  return count\n",
        "\n",
        "def num_punctuation(x):\n",
        "  # get count the number of punctuation symbols\n",
        "  _, count = re.subn(r'\\W', '', x)\n",
        "  return count"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "innAg2YOhKmk"
      },
      "source": [
        "Additional feature columns will be added to the DataFrame, and then the set will\r\n",
        "be split into test and train sets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaPsdhs6mkd_"
      },
      "source": [
        "df['Capitals'] = df['Message'].apply(num_capitals)\n",
        "df['Punctuation'] = df['Message'].apply(num_punctuation)\n",
        "df['Length'] = df['Message'].apply(message_length)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "8fwkbELUilzd",
        "outputId": "22f84cb1-c4f1-4d62-ae52-3d232ec021c9"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Spam</th>\n",
              "      <th>Message</th>\n",
              "      <th>Capitals</th>\n",
              "      <th>Punctuation</th>\n",
              "      <th>Length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>3</td>\n",
              "      <td>28</td>\n",
              "      <td>111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>10</td>\n",
              "      <td>33</td>\n",
              "      <td>155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>2</td>\n",
              "      <td>14</td>\n",
              "      <td>61</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Spam                                            Message  ...  Punctuation  Length\n",
              "0     0  Go until jurong point, crazy.. Available only ...  ...           28     111\n",
              "1     0                      Ok lar... Joking wif u oni...  ...           11      29\n",
              "2     1  Free entry in 2 a wkly comp to win FA Cup fina...  ...           33     155\n",
              "3     0  U dun say so early hor... U c already then say...  ...           16      49\n",
              "4     0  Nah I don't think he goes to usf, he lives aro...  ...           14      61\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "7Oy5mN8nnrde",
        "outputId": "fc56b83f-c755-41a0-fe69-aeb0c0f6c78a"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Spam</th>\n",
              "      <th>Capitals</th>\n",
              "      <th>Punctuation</th>\n",
              "      <th>Length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>5574.000000</td>\n",
              "      <td>5574.000000</td>\n",
              "      <td>5574.000000</td>\n",
              "      <td>5574.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.134015</td>\n",
              "      <td>5.621636</td>\n",
              "      <td>18.942591</td>\n",
              "      <td>80.443488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.340699</td>\n",
              "      <td>11.683233</td>\n",
              "      <td>14.825994</td>\n",
              "      <td>59.841746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>36.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>61.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>27.000000</td>\n",
              "      <td>122.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>129.000000</td>\n",
              "      <td>253.000000</td>\n",
              "      <td>910.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Spam     Capitals  Punctuation       Length\n",
              "count  5574.000000  5574.000000  5574.000000  5574.000000\n",
              "mean      0.134015     5.621636    18.942591    80.443488\n",
              "std       0.340699    11.683233    14.825994    59.841746\n",
              "min       0.000000     0.000000     0.000000     2.000000\n",
              "25%       0.000000     1.000000     8.000000    36.000000\n",
              "50%       0.000000     2.000000    15.000000    61.000000\n",
              "75%       0.000000     4.000000    27.000000   122.000000\n",
              "max       1.000000   129.000000   253.000000   910.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGHh7BGZhiu_"
      },
      "source": [
        "Now let's split the dataset into training and test sets, with\r\n",
        "80% of the records in the training set and the rest in the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RX7Ne4NLoPIE"
      },
      "source": [
        "train=df.sample(frac=0.8,random_state=42) #random state is a seed value\n",
        "test=df.drop(train.index)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "FemsnVTto-c6",
        "outputId": "cfb6acb5-1cda-4e8a-86eb-81789210a0b5"
      },
      "source": [
        "train.describe()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Spam</th>\n",
              "      <th>Capitals</th>\n",
              "      <th>Punctuation</th>\n",
              "      <th>Length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4459.000000</td>\n",
              "      <td>4459.000000</td>\n",
              "      <td>4459.000000</td>\n",
              "      <td>4459.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.132765</td>\n",
              "      <td>5.519399</td>\n",
              "      <td>18.886522</td>\n",
              "      <td>80.316439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.339359</td>\n",
              "      <td>11.405424</td>\n",
              "      <td>14.602023</td>\n",
              "      <td>59.346407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>35.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>61.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>27.000000</td>\n",
              "      <td>122.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>129.000000</td>\n",
              "      <td>253.000000</td>\n",
              "      <td>910.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Spam     Capitals  Punctuation       Length\n",
              "count  4459.000000  4459.000000  4459.000000  4459.000000\n",
              "mean      0.132765     5.519399    18.886522    80.316439\n",
              "std       0.339359    11.405424    14.602023    59.346407\n",
              "min       0.000000     0.000000     0.000000     2.000000\n",
              "25%       0.000000     1.000000     8.000000    35.000000\n",
              "50%       0.000000     2.000000    15.000000    61.000000\n",
              "75%       0.000000     4.000000    27.000000   122.000000\n",
              "max       1.000000   129.000000   253.000000   910.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "CEe1TMcApCYS",
        "outputId": "0d4ca0d5-05cc-4976-b4a1-5258983f48c7"
      },
      "source": [
        "test.describe()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Spam</th>\n",
              "      <th>Capitals</th>\n",
              "      <th>Punctuation</th>\n",
              "      <th>Length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1115.000000</td>\n",
              "      <td>1115.000000</td>\n",
              "      <td>1115.000000</td>\n",
              "      <td>1115.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.139013</td>\n",
              "      <td>6.030493</td>\n",
              "      <td>19.166816</td>\n",
              "      <td>80.951570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.346116</td>\n",
              "      <td>12.731059</td>\n",
              "      <td>15.694599</td>\n",
              "      <td>61.807655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>36.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>61.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>123.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>127.000000</td>\n",
              "      <td>195.000000</td>\n",
              "      <td>790.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Spam     Capitals  Punctuation       Length\n",
              "count  1115.000000  1115.000000  1115.000000  1115.000000\n",
              "mean      0.139013     6.030493    19.166816    80.951570\n",
              "std       0.346116    12.731059    15.694599    61.807655\n",
              "min       0.000000     0.000000     0.000000     2.000000\n",
              "25%       0.000000     1.000000     8.000000    36.000000\n",
              "50%       0.000000     2.000000    15.000000    61.000000\n",
              "75%       0.000000     4.000000    28.000000   123.000000\n",
              "max       1.000000   127.000000   195.000000   790.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EZeaklCiAEn"
      },
      "source": [
        "Further more, labels will be removed from both the training and test sets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnXJl3AgiBDu"
      },
      "source": [
        "x_train = train[['Length', 'Punctuation', 'Capitals']]\r\n",
        "y_train = train[['Spam']]\r\n",
        "\r\n",
        "x_test = test[['Length', 'Punctuation', 'Capitals']]\r\n",
        "y_test = test[['Spam']]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EB64N3y9iMSs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "outputId": "9bbd0914-5e16-4461-d066-eb38568a94f6"
      },
      "source": [
        "x_train.describe()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Length</th>\n",
              "      <th>Punctuation</th>\n",
              "      <th>Capitals</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4459.000000</td>\n",
              "      <td>4459.000000</td>\n",
              "      <td>4459.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>80.316439</td>\n",
              "      <td>18.886522</td>\n",
              "      <td>5.519399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>59.346407</td>\n",
              "      <td>14.602023</td>\n",
              "      <td>11.405424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>35.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>61.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>122.000000</td>\n",
              "      <td>27.000000</td>\n",
              "      <td>4.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>910.000000</td>\n",
              "      <td>253.000000</td>\n",
              "      <td>129.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Length  Punctuation     Capitals\n",
              "count  4459.000000  4459.000000  4459.000000\n",
              "mean     80.316439    18.886522     5.519399\n",
              "std      59.346407    14.602023    11.405424\n",
              "min       2.000000     0.000000     0.000000\n",
              "25%      35.000000     8.000000     1.000000\n",
              "50%      61.000000    15.000000     2.000000\n",
              "75%     122.000000    27.000000     4.000000\n",
              "max     910.000000   253.000000   129.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7AdstxMiS2l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "outputId": "7d11ca27-41b7-4157-e85d-e0063ad4dc00"
      },
      "source": [
        "x_test.describe()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Length</th>\n",
              "      <th>Punctuation</th>\n",
              "      <th>Capitals</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1115.000000</td>\n",
              "      <td>1115.000000</td>\n",
              "      <td>1115.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>80.951570</td>\n",
              "      <td>19.166816</td>\n",
              "      <td>6.030493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>61.807655</td>\n",
              "      <td>15.694599</td>\n",
              "      <td>12.731059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>36.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>61.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>123.000000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>4.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>790.000000</td>\n",
              "      <td>195.000000</td>\n",
              "      <td>127.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Length  Punctuation     Capitals\n",
              "count  1115.000000  1115.000000  1115.000000\n",
              "mean     80.951570    19.166816     6.030493\n",
              "std      61.807655    15.694599    12.731059\n",
              "min       2.000000     0.000000     0.000000\n",
              "25%      36.000000     8.000000     1.000000\n",
              "50%      61.000000    15.000000     2.000000\n",
              "75%     123.000000    28.000000     4.000000\n",
              "max     790.000000   195.000000   127.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9rSWD4i1X3s"
      },
      "source": [
        "### Modeling case normalized data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s74YoH-nSeO"
      },
      "source": [
        "We will use a very simple model, as the objective is to show different basic NLP data processing techniques more than modeling. Here, we want to see if three simple features can aid in the classification of spam. As more features are added, passing them through the same model will help in seeing if the\r\n",
        "featurization aids or hampers the accuracy of the classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WU6sxf2qcZd"
      },
      "source": [
        "# Basic 1-layer neural network model for evaluation\n",
        "def make_model(input_dims=3, num_units=12):\n",
        "  model = tf.keras.Sequential()\n",
        "\n",
        "  # Adds a densely-connected layer with 12 units to the model:\n",
        "  model.add(tf.keras.layers.Dense(num_units, \n",
        "                                  input_dim=input_dims, \n",
        "                                  activation='relu'))\n",
        "\n",
        "  # Add a sigmoid layer with a binary output unit:\n",
        "  model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_XRB02Rn0zP"
      },
      "source": [
        "This model uses binary cross-entropy for computing loss and the Adam optimizer\r\n",
        "for training. The key metric, given that this is a binary classification problem, is accuracy.\r\n",
        "\r\n",
        "We can train our simple baseline model with only three features like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz1aTS9LuFpF"
      },
      "source": [
        "model = make_model()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otOL712wu4tW",
        "outputId": "37ddaadb-d504-4146-f0a3-06b8a9ecf0bf"
      },
      "source": [
        "model.fit(x_train, y_train, epochs=10, batch_size=10)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "446/446 [==============================] - 3s 2ms/step - loss: 5.0967 - accuracy: 0.5424\n",
            "Epoch 2/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.4894 - accuracy: 0.8783\n",
            "Epoch 3/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.3151 - accuracy: 0.9097\n",
            "Epoch 4/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.2531 - accuracy: 0.9230\n",
            "Epoch 5/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.2201 - accuracy: 0.9280\n",
            "Epoch 6/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.2215 - accuracy: 0.9262\n",
            "Epoch 7/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.2137 - accuracy: 0.9315\n",
            "Epoch 8/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.1969 - accuracy: 0.9385\n",
            "Epoch 9/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.2120 - accuracy: 0.9254\n",
            "Epoch 10/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.1912 - accuracy: 0.9307\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc0c007d748>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJf7Xza5oRAD"
      },
      "source": [
        "This is not bad as our three simple features help us get to 93% accuracy. A quick check shows that there are 592 spam messages in the test set, out of a total of 4,459. So, this model is doing better than a very simple model that guesses everything as not spam.\r\n",
        "\r\n",
        "That model would have an accuracy of 87%. This number may be\r\n",
        "surprising but is fairly common in classification problems where there is a severe class imbalance in the data. Evaluating it on the training set gives an accuracy of around 93.27%:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svFEbDWzccXV",
        "outputId": "b56b89be-0653-4db6-d352-c9ae98151e15"
      },
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "35/35 [==============================] - 0s 1ms/step - loss: 0.1974 - accuracy: 0.9327\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.19739584624767303, 0.9327354431152344]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Vo6YuM0p960"
      },
      "source": [
        "Please note that the actual performance you see may be slightly different due to the data splits and computational vagaries. \r\n",
        "\r\n",
        "A quick verification can be performed by plotting the confusion matrix to see the performance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Aw3qskjDC-j",
        "outputId": "e59833df-3283-411c-d29a-3211c5bd1d29"
      },
      "source": [
        "y_train_pred = model.predict_classes(x_train)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c73dfRXaFD3F",
        "outputId": "6fd33e34-5a41-47b3-b556-146887a8fc7c"
      },
      "source": [
        "# confusion matrix\n",
        "tf.math.confusion_matrix(tf.constant(y_train.Spam), y_train_pred)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
              "array([[3777,   90],\n",
              "       [ 186,  406]], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdMnEPdqrsaq"
      },
      "source": [
        "This shows that 3,666 out of 3,867 regular messages were classified correctly, while 353 out of 592 spam messages were classified correctly. Again, you may get a slightly different result.\r\n",
        "\r\n",
        "|  | **Predicted Not Spam** | **Predicted Spam** |\r\n",
        "| --- | --- | --- |\r\n",
        "| **Actual Not Spam** | 3777 | 90 |\r\n",
        "| **Actual Spam** | 186 | 406 |\r\n",
        "\r\n",
        "We can get calculation as follow:\r\n",
        "\r\n",
        "|  | **Predicted Not Spam** | **Predicted Spam** | |\r\n",
        "| --- | --- | --- | |\r\n",
        "| **Actual Not Spam** | 3777 | 90 | 3777 + 90 = 3867 |\r\n",
        "| **Actual Spam**     | 186  | 406 |  186 + 406 = 592  |\r\n",
        "\r\n",
        "So confusion matrix show us that if we reduce the value 201 and 239 then accuracy would be increased."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmcZKcJqGiAK",
        "outputId": "cddcd73a-92ca-4139-d843-893445032b65"
      },
      "source": [
        "sum(y_train_pred)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([496], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUBeCm5eDduc",
        "outputId": "633fece0-bb50-45c3-fd76-5e6fb6e352c5"
      },
      "source": [
        "y_test_pred = model.predict_classes(x_test)\n",
        "tf.math.confusion_matrix(tf.constant(y_test.Spam), y_test_pred)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
              "array([[943,  17],\n",
              "       [ 58,  97]], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZG-KrlJ_jgaB"
      },
      "source": [
        "### Excersize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmOUztFG3hA0"
      },
      "source": [
        "To test the value of the features, try re-running the model by removing one of the features, such as punctuation or a number of capital letters, to get a sense of their contribution to the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w73qPjPo6O7u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "outputId": "ba7e9aa1-3079-458e-cb08-d6c993b4eaff"
      },
      "source": [
        "x_train = train[['Length', 'Punctuation']]\r\n",
        "y_train = train[['Spam']]\r\n",
        "\r\n",
        "x_test = test[['Length', 'Punctuation']]\r\n",
        "y_test = test[['Spam']]\r\n",
        "\r\n",
        "x_train.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Length</th>\n",
              "      <th>Punctuation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4459.000000</td>\n",
              "      <td>4459.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>80.316439</td>\n",
              "      <td>18.886522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>59.346407</td>\n",
              "      <td>14.602023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>35.000000</td>\n",
              "      <td>8.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>61.000000</td>\n",
              "      <td>15.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>122.000000</td>\n",
              "      <td>27.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>910.000000</td>\n",
              "      <td>253.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Length  Punctuation\n",
              "count  4459.000000  4459.000000\n",
              "mean     80.316439    18.886522\n",
              "std      59.346407    14.602023\n",
              "min       2.000000     0.000000\n",
              "25%      35.000000     8.000000\n",
              "50%      61.000000    15.000000\n",
              "75%     122.000000    27.000000\n",
              "max     910.000000   253.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9ZY2PGA7AJ5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "outputId": "b42b636a-44fd-49aa-e8f5-965bd89959a5"
      },
      "source": [
        "x_test.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Length</th>\n",
              "      <th>Punctuation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1115.000000</td>\n",
              "      <td>1115.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>80.951570</td>\n",
              "      <td>19.166816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>61.807655</td>\n",
              "      <td>15.694599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>36.000000</td>\n",
              "      <td>8.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>61.000000</td>\n",
              "      <td>15.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>123.000000</td>\n",
              "      <td>28.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>790.000000</td>\n",
              "      <td>195.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Length  Punctuation\n",
              "count  1115.000000  1115.000000\n",
              "mean     80.951570    19.166816\n",
              "std      61.807655    15.694599\n",
              "min       2.000000     0.000000\n",
              "25%      36.000000     8.000000\n",
              "50%      61.000000    15.000000\n",
              "75%     123.000000    28.000000\n",
              "max     790.000000   195.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqzLS6Se6cJ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56fc98e3-ebb4-45cd-9810-8d8dc1c8812a"
      },
      "source": [
        "model1 = make_model(input_dims=2)\r\n",
        "model1.fit(x_train, y_train, epochs=10, batch_size=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 7.6084 - accuracy: 0.4583\n",
            "Epoch 2/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.4482 - accuracy: 0.8703\n",
            "Epoch 3/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.3949 - accuracy: 0.8707\n",
            "Epoch 4/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.3514 - accuracy: 0.8709\n",
            "Epoch 5/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.3267 - accuracy: 0.8774\n",
            "Epoch 6/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.3018 - accuracy: 0.8742\n",
            "Epoch 7/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.2707 - accuracy: 0.9007\n",
            "Epoch 8/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.2860 - accuracy: 0.8865\n",
            "Epoch 9/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.2889 - accuracy: 0.8846\n",
            "Epoch 10/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.2662 - accuracy: 0.8905\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb3a40d9198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yl3fHX2v6kWu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebb5ab5e-3290-4ed0-eb98-c3107bba4dd6"
      },
      "source": [
        "model1.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "35/35 [==============================] - 0s 1ms/step - loss: 0.2617 - accuracy: 0.8942\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.26170480251312256, 0.8941704034805298]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbHgc-4o6o7_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10251b53-3cc4-4e64-baf9-0a3160988fef"
      },
      "source": [
        "y_train_pred = model1.predict_classes(x_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0tXsgdK669a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cf6d575-4bc3-46ab-c9f2-cf0a6a65fd19"
      },
      "source": [
        "# confusion matrix\r\n",
        "tf.math.confusion_matrix(tf.constant(y_train.Spam), y_train_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
              "array([[3780,   87],\n",
              "       [ 385,  207]], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erz6fMIh7inz"
      },
      "source": [
        "Now trying to remove Punctuation letters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPY5twmY7i-a"
      },
      "source": [
        "x_train = train[['Length', 'Capitals']]\r\n",
        "y_train = train[['Spam']]\r\n",
        "\r\n",
        "x_test = test[['Length', 'Capitals']]\r\n",
        "y_test = test[['Spam']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMjL2V7Q7oAK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b7b80ef-eed6-4493-a8d6-0e2e69ce0c4f"
      },
      "source": [
        "model2 = make_model(input_dims=2)\r\n",
        "model2.fit(x_train, y_train, epochs=10, batch_size=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 1.2122 - accuracy: 0.8613\n",
            "Epoch 2/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.3679 - accuracy: 0.9050\n",
            "Epoch 3/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.3040 - accuracy: 0.9133\n",
            "Epoch 4/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.2656 - accuracy: 0.9222\n",
            "Epoch 5/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.2652 - accuracy: 0.9213\n",
            "Epoch 6/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.2706 - accuracy: 0.9097\n",
            "Epoch 7/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.2446 - accuracy: 0.9219\n",
            "Epoch 8/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.2538 - accuracy: 0.9145\n",
            "Epoch 9/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.2415 - accuracy: 0.9150\n",
            "Epoch 10/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.2343 - accuracy: 0.9198\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb3906cf2b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5R3euiud7qsY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bea7e0e-2b79-447c-edc6-9a6073801bcf"
      },
      "source": [
        "model2.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "35/35 [==============================] - 0s 1ms/step - loss: 0.2812 - accuracy: 0.9085\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2811644971370697, 0.9085201621055603]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlNcb5VU7sqD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb868559-f9c2-4638-8f26-9bff2797bb24"
      },
      "source": [
        "y_train_pred = model2.predict_classes(x_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgAOpxmp7tJy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ad4dfe0-5073-4344-b606-e60ace3a3006"
      },
      "source": [
        "# confusion matrix\r\n",
        "tf.math.confusion_matrix(tf.constant(y_train.Spam), y_train_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
              "array([[3684,  183],\n",
              "       [ 171,  421]], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "warQbntF8NMc"
      },
      "source": [
        "We observe that removing one of the features punctuation letters, It contribute to the model accuracy by increasing upto 90%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY-ssfsAOusL"
      },
      "source": [
        "## Tokenization normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_Ep_y1hmR29"
      },
      "source": [
        "This step takes a piece of text and converts it into a list of tokens. If the input is a sentence, then separating the words would be an example of tokenization. Depending on the model, different granularities can be chosen. At the lowest level, each character could become a token. In some cases, entire sentences of paragraphs can be considered as a token:\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/advanced-nlp-with-tensorflow-2/sentence-tokenizing.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "The preceding diagram shows two ways a sentence can be tokenized. One way to\r\n",
        "tokenize is to chop a sentence into words. Another way is to chop into individual characters. However, this can be a complex proposition in some languages such as Japanese and Mandarin.\r\n",
        "\r\n",
        "\r\n",
        "Many languages use a word separator, a space, to separate words. This makes the\r\n",
        "task of tokenizing on words trivial. However, there are other languages that do not use any markers or separators between words. Some examples of such languages are Japanese and Chinese. In such languages, the task is referred to as segmentation.\r\n",
        "\r\n",
        "Fortunately, most languages are not as complex as Japanese and use spaces to\r\n",
        "separate words. In Python, splitting by spaces is trivial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MWyaX6IPk2l",
        "outputId": "6161b9e8-80df-491d-f381-bdf95cabf004"
      },
      "source": [
        "sentence = 'Go until jurong point, crazy.. Available only in bugis n great world'\n",
        "sentence.split()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Go',\n",
              " 'until',\n",
              " 'jurong',\n",
              " 'point,',\n",
              " 'crazy..',\n",
              " 'Available',\n",
              " 'only',\n",
              " 'in',\n",
              " 'bugis',\n",
              " 'n',\n",
              " 'great',\n",
              " 'world']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKV-uwGNppag"
      },
      "source": [
        "The two lines(`point,` and `crazy..`) in the preceding output show that the naÃ¯ve approach in Python will result in punctuation being included in the words, among other issues. Consequently, this step is done through a library like StanfordNLP.\r\n",
        "\r\n",
        "This package provides capabilities for tokenization, POS tagging, and lemmatization out of the box. To start with tokenization, we instantiate a pipeline and tokenize a sample text to see how this works:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RriSmiOlqMW3",
        "outputId": "9425a174-e123-4200-c529-484bd632496d"
      },
      "source": [
        "en = snlp.Pipeline(lang=\"en\", processors=\"tokenize\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Use device: gpu\n",
            "---\n",
            "Loading: tokenize\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "Done loading processors!\n",
            "---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKF8dMc8tWlP"
      },
      "source": [
        "For now, only tokenization of text is desired, so only the tokenizer is used:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qISguLeetXCe",
        "outputId": "8379b6d7-29aa-41a2-dc91-8cc470a97960"
      },
      "source": [
        "tokenized = en(sentence)\r\n",
        "len(tokenized.sentences)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LB3TeWujtmIo"
      },
      "source": [
        "This shows that the tokenizer correctly divided the text into two sentences.\r\n",
        "\r\n",
        "To investigate what words were removed, the following code can be used:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IW9jwgqEtmx6",
        "outputId": "736226a4-4810-4c31-cd80-7367100202f6"
      },
      "source": [
        "for snt in tokenized.sentences:\r\n",
        "  for word in snt.tokens:\r\n",
        "    print(word.text)\r\n",
        "  print(\"<End of Sentence>\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go\n",
            "until\n",
            "jurong\n",
            "point\n",
            ",\n",
            "crazy\n",
            "..\n",
            "<End of Sentence>\n",
            "Available\n",
            "only\n",
            "in\n",
            "bugis\n",
            "n\n",
            "great\n",
            "world\n",
            "<End of Sentence>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WJVunh4t9dA"
      },
      "source": [
        "Punctuation marks were separated out into their own words. Text was split into multiple sentences. This is an improvement over only using spaces to split. In some applications, removal of punctuation may be required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5NI7lL_aPrR"
      },
      "source": [
        "### Japanese Tokenization Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7BlmnJdujAR"
      },
      "source": [
        "Consider the preceding example of Japanese. To see the performance of StanfordNLP on Japanese tokenization, the following piece of code can be used:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twQsK3NSRh3e",
        "outputId": "6d90012e-7548-4951-9612-ceaf976676fc"
      },
      "source": [
        "jp = snlp.download('ja')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using the default treebank \"ja_gsd\" for language \"ja\".\n",
            "Would you like to download the models for: ja_gsd now? (Y/n)\n",
            "Y\n",
            "\n",
            "Default download directory: /root/stanfordnlp_resources\n",
            "Hit enter to continue or type an alternate directory.\n",
            "\n",
            "\n",
            "Downloading models for: ja_gsd\n",
            "Download location: /root/stanfordnlp_resources/ja_gsd_models.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 219M/219M [00:13<00:00, 15.9MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Download complete.  Models saved to: /root/stanfordnlp_resources/ja_gsd_models.zip\n",
            "Extracting models file for: ja_gsd\n",
            "Cleaning up...Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axyFewm6u768"
      },
      "source": [
        "Next, a Japanese pipeline will be instantiated and the words will be processed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VM_AJ4mCS2ja",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b4605bf-144b-476e-d028-d2e6588681a2"
      },
      "source": [
        "jp = snlp.Pipeline(lang=\"ja\", processors=\"tokenize\")\r\n",
        "jp_line = jp(\"é¸æŒ™ç®¡ç†å§”å“¡ä¼š\")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Use device: gpu\n",
            "---\n",
            "Loading: tokenize\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/ja_gsd_models/ja_gsd_tokenizer.pt', 'lang': 'ja', 'shorthand': 'ja_gsd', 'mode': 'predict'}\n",
            "Done loading processors!\n",
            "---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EbAZGWLvEHR"
      },
      "source": [
        "You may recall that the Japanese text reads Election Administration Committee.\r\n",
        "Correct tokenization should produce three words, where first two should be two\r\n",
        "characters each, and the last word is three characters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMo9DqvMTzX_",
        "outputId": "caef0249-e843-4bcc-d327-c281813655f8"
      },
      "source": [
        "for snt in jp_line.sentences:\n",
        "  for word in snt.tokens:\n",
        "    print(word.text)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "é¸æŒ™\n",
            "ç®¡ç†\n",
            "å§”å“¡ä¼š\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NAW2TRM2HSl"
      },
      "source": [
        "This matches the expected output. StanfordNLP supports 53 languages, so the same\r\n",
        "code can be used for tokenizing any language that is supported.\r\n",
        "\r\n",
        "Coming back to the spam detection example, a new feature can be implemented that\r\n",
        "counts the number of words in the message using this tokenization functionality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nKfjQeOaWtz"
      },
      "source": [
        "### Modeling tokenized data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZghEI-h2cBL"
      },
      "source": [
        "It is possible that spam messages have different numbers of words than regular\r\n",
        "messages. The first step is to define a method to compute the number of words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBK3MokpYa6n"
      },
      "source": [
        "def word_counts(x, pipeline=en):\n",
        "  doc = pipeline(x)\n",
        "  count = sum( [ len(sentence.tokens) for sentence in doc.sentences] )\n",
        "  return count"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQf6MQNmbazh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "882546bc-a884-4e92-ae16-f98c01ba5981"
      },
      "source": [
        "en = snlp.Pipeline(lang='en', processors='tokenize')\n",
        "df['Words'] = df['Message'].apply(word_counts)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Use device: gpu\n",
            "---\n",
            "Loading: tokenize\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "Done loading processors!\n",
            "---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "JGgd-Us74l5P",
        "outputId": "4f0a4f1a-dd07-4f22-b779-f9a43ee602d3"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Spam</th>\n",
              "      <th>Message</th>\n",
              "      <th>Capitals</th>\n",
              "      <th>Punctuation</th>\n",
              "      <th>Length</th>\n",
              "      <th>Words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>3</td>\n",
              "      <td>28</td>\n",
              "      <td>111</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "      <td>29</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>10</td>\n",
              "      <td>33</td>\n",
              "      <td>155</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>49</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>2</td>\n",
              "      <td>14</td>\n",
              "      <td>61</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Spam                                            Message  ...  Length  Words\n",
              "0     0  Go until jurong point, crazy.. Available only ...  ...     111     24\n",
              "1     0                      Ok lar... Joking wif u oni...  ...      29      8\n",
              "2     1  Free entry in 2 a wkly comp to win FA Cup fina...  ...     155     34\n",
              "3     0  U dun say so early hor... U c already then say...  ...      49     13\n",
              "4     0  Nah I don't think he goes to usf, he lives aro...  ...      61     15\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiIkXlIA32J_"
      },
      "source": [
        "Next, using the train and test splits, add a column for the word count feature:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-pqZ1a3dOg2"
      },
      "source": [
        "#train=df.sample(frac=0.8,random_state=42) #random state is a seed value\n",
        "#test=df.drop(train.index)\n",
        "\n",
        "train['Words'] = train['Message'].apply(word_counts)\n",
        "test['Words'] = test['Message'].apply(word_counts)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lruns2ddQzT"
      },
      "source": [
        "x_train = train[['Length', 'Punctuation', 'Capitals', 'Words']]\n",
        "y_train = train[['Spam']]\n",
        "\n",
        "x_test = test[['Length', 'Punctuation', 'Capitals' , 'Words']]\n",
        "y_test = test[['Spam']]"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8So4_u34daa5",
        "outputId": "fc167aa1-6ef9-40c1-a5c8-54a90c1e791b"
      },
      "source": [
        "model = make_model(input_dims=4)\r\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=10)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.8421 - accuracy: 0.7806\n",
            "Epoch 2/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.3981 - accuracy: 0.8814\n",
            "Epoch 3/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.3265 - accuracy: 0.8992\n",
            "Epoch 4/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.2656 - accuracy: 0.9247\n",
            "Epoch 5/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.2493 - accuracy: 0.9246\n",
            "Epoch 6/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.2465 - accuracy: 0.9153\n",
            "Epoch 7/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.2277 - accuracy: 0.9235\n",
            "Epoch 8/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.2239 - accuracy: 0.9243\n",
            "Epoch 9/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.2106 - accuracy: 0.9260\n",
            "Epoch 10/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.2069 - accuracy: 0.9253\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc075581e10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwYOE-oB5EEz"
      },
      "source": [
        "There is only a marginal improvement in accuracy. One hypothesis is that the\r\n",
        "number of words is not useful. It would be useful if the average number of words in spam messages were smaller or larger than regular messages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iw__r38sdv60",
        "outputId": "17a53ffb-90a6-4e2b-aaf7-f4c04a247725"
      },
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2073 - accuracy: 0.9211\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2072787880897522, 0.921076238155365]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pA9EZl155Nfw"
      },
      "source": [
        "Using pandas, this can be quickly verified:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "BZUvMagI5O5G",
        "outputId": "3d01df0a-5c1f-4694-a5bc-47f690ea3670"
      },
      "source": [
        "train.loc[train.Spam == 1].describe()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Spam</th>\n",
              "      <th>Capitals</th>\n",
              "      <th>Punctuation</th>\n",
              "      <th>Length</th>\n",
              "      <th>Words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>592.0</td>\n",
              "      <td>592.000000</td>\n",
              "      <td>592.000000</td>\n",
              "      <td>592.000000</td>\n",
              "      <td>592.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.0</td>\n",
              "      <td>15.320946</td>\n",
              "      <td>29.086149</td>\n",
              "      <td>138.856419</td>\n",
              "      <td>29.511824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.0</td>\n",
              "      <td>11.635105</td>\n",
              "      <td>7.083572</td>\n",
              "      <td>28.079980</td>\n",
              "      <td>7.474256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>132.000000</td>\n",
              "      <td>26.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>149.000000</td>\n",
              "      <td>30.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>34.000000</td>\n",
              "      <td>157.000000</td>\n",
              "      <td>35.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.0</td>\n",
              "      <td>128.000000</td>\n",
              "      <td>49.000000</td>\n",
              "      <td>197.000000</td>\n",
              "      <td>49.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Spam    Capitals  Punctuation      Length       Words\n",
              "count  592.0  592.000000   592.000000  592.000000  592.000000\n",
              "mean     1.0   15.320946    29.086149  138.856419   29.511824\n",
              "std      0.0   11.635105     7.083572   28.079980    7.474256\n",
              "min      1.0    0.000000     2.000000   13.000000    3.000000\n",
              "25%      1.0    7.000000    26.000000  132.000000   26.000000\n",
              "50%      1.0   14.000000    30.000000  149.000000   30.000000\n",
              "75%      1.0   21.000000    34.000000  157.000000   35.000000\n",
              "max      1.0  128.000000    49.000000  197.000000   49.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8ctKayW5cWM"
      },
      "source": [
        "Let's compare the preceding results to the statistics for regular messages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "43pbHPeY5cwH",
        "outputId": "22f9ca03-22b4-46d9-a990-a451acde55dc"
      },
      "source": [
        "train.loc[train.Spam == 0].describe()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Spam</th>\n",
              "      <th>Capitals</th>\n",
              "      <th>Punctuation</th>\n",
              "      <th>Length</th>\n",
              "      <th>Words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>3867.0</td>\n",
              "      <td>3867.000000</td>\n",
              "      <td>3867.000000</td>\n",
              "      <td>3867.000000</td>\n",
              "      <td>3867.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.0</td>\n",
              "      <td>4.018878</td>\n",
              "      <td>17.325058</td>\n",
              "      <td>71.354538</td>\n",
              "      <td>17.344194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.0</td>\n",
              "      <td>10.599291</td>\n",
              "      <td>14.826644</td>\n",
              "      <td>57.755351</td>\n",
              "      <td>13.811278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>33.000000</td>\n",
              "      <td>8.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>53.000000</td>\n",
              "      <td>13.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>92.000000</td>\n",
              "      <td>22.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.0</td>\n",
              "      <td>129.000000</td>\n",
              "      <td>253.000000</td>\n",
              "      <td>910.000000</td>\n",
              "      <td>209.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Spam     Capitals  Punctuation       Length        Words\n",
              "count  3867.0  3867.000000  3867.000000  3867.000000  3867.000000\n",
              "mean      0.0     4.018878    17.325058    71.354538    17.344194\n",
              "std       0.0    10.599291    14.826644    57.755351    13.811278\n",
              "min       0.0     0.000000     0.000000     2.000000     1.000000\n",
              "25%       0.0     1.000000     8.000000    33.000000     8.000000\n",
              "50%       0.0     2.000000    13.000000    53.000000    13.000000\n",
              "75%       0.0     3.000000    23.000000    92.000000    22.000000\n",
              "max       0.0   129.000000   253.000000   910.000000   209.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvtjp9cL50sy"
      },
      "source": [
        "Some interesting patterns can quickly be seen. Spam messages usually have much\r\n",
        "less deviation from the mean. Focus on the Capitals feature column. It shows that regular messages use far fewer capitals than spam messages.\r\n",
        "\r\n",
        "This quick check yields an indication as to why adding the word features wasn't that useful. However, there are a couple of things to consider still. \r\n",
        "\r\n",
        "First, the tokenization model split out punctuation marks as words. Ideally, these words should be removed from the word counts as the punctuation feature is showing that spam messages use a lot more punctuation characters.\r\n",
        "\r\n",
        "Secondly, languages have some common words that are usually excluded. This is called stop word removal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VH43JjrwS9Ta"
      },
      "source": [
        "## Stop Word Removal normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "ukeJuqpdmKI2",
        "outputId": "85481878-bfdb-48c2-b224-5b14f0c21e6c"
      },
      "source": [
        "!pip install stopwordsiso"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stopwordsiso\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/03/4c5f24b654bb9459f81aa5c1b60b094b804286b99dca9f2e116c9eb01ac8/stopwordsiso-0.6.1-py3-none-any.whl (73kB)\n",
            "\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                           | 10kB 19.4MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                       | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 30kB 2.1MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰              | 40kB 2.4MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž         | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81kB 2.3MB/s \n",
            "\u001b[?25hInstalling collected packages: stopwordsiso\n",
            "Successfully installed stopwordsiso-0.6.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "E-kxi8XBTA1r",
        "outputId": "ece1e1cb-6cf9-4cf2-a4e9-6f53eb6d623d"
      },
      "source": [
        "import stopwordsiso as stopwords\n",
        "\n",
        "stopwords.langs()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'af',\n",
              " 'ar',\n",
              " 'bg',\n",
              " 'bn',\n",
              " 'br',\n",
              " 'ca',\n",
              " 'cs',\n",
              " 'da',\n",
              " 'de',\n",
              " 'el',\n",
              " 'en',\n",
              " 'eo',\n",
              " 'es',\n",
              " 'et',\n",
              " 'eu',\n",
              " 'fa',\n",
              " 'fi',\n",
              " 'fr',\n",
              " 'ga',\n",
              " 'gl',\n",
              " 'gu',\n",
              " 'ha',\n",
              " 'he',\n",
              " 'hi',\n",
              " 'hr',\n",
              " 'hu',\n",
              " 'hy',\n",
              " 'id',\n",
              " 'it',\n",
              " 'ja',\n",
              " 'ko',\n",
              " 'ku',\n",
              " 'la',\n",
              " 'lt',\n",
              " 'lv',\n",
              " 'mr',\n",
              " 'ms',\n",
              " 'nl',\n",
              " 'no',\n",
              " 'pl',\n",
              " 'pt',\n",
              " 'ro',\n",
              " 'ru',\n",
              " 'sk',\n",
              " 'sl',\n",
              " 'so',\n",
              " 'st',\n",
              " 'sv',\n",
              " 'sw',\n",
              " 'th',\n",
              " 'tl',\n",
              " 'tr',\n",
              " 'uk',\n",
              " 'ur',\n",
              " 'vi',\n",
              " 'yo',\n",
              " 'zh',\n",
              " 'zu'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RE093gWDT57H",
        "outputId": "983b40db-d22e-4c0f-94d9-715d1921c3e9"
      },
      "source": [
        "sorted(stopwords.stopwords('en'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"'ll\",\n",
              " \"'tis\",\n",
              " \"'twas\",\n",
              " \"'ve\",\n",
              " '10',\n",
              " '39',\n",
              " 'a',\n",
              " \"a's\",\n",
              " 'able',\n",
              " 'ableabout',\n",
              " 'about',\n",
              " 'above',\n",
              " 'abroad',\n",
              " 'abst',\n",
              " 'accordance',\n",
              " 'according',\n",
              " 'accordingly',\n",
              " 'across',\n",
              " 'act',\n",
              " 'actually',\n",
              " 'ad',\n",
              " 'added',\n",
              " 'adj',\n",
              " 'adopted',\n",
              " 'ae',\n",
              " 'af',\n",
              " 'affected',\n",
              " 'affecting',\n",
              " 'affects',\n",
              " 'after',\n",
              " 'afterwards',\n",
              " 'ag',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ago',\n",
              " 'ah',\n",
              " 'ahead',\n",
              " 'ai',\n",
              " \"ain't\",\n",
              " 'aint',\n",
              " 'al',\n",
              " 'all',\n",
              " 'allow',\n",
              " 'allows',\n",
              " 'almost',\n",
              " 'alone',\n",
              " 'along',\n",
              " 'alongside',\n",
              " 'already',\n",
              " 'also',\n",
              " 'although',\n",
              " 'always',\n",
              " 'am',\n",
              " 'amid',\n",
              " 'amidst',\n",
              " 'among',\n",
              " 'amongst',\n",
              " 'amoungst',\n",
              " 'amount',\n",
              " 'an',\n",
              " 'and',\n",
              " 'announce',\n",
              " 'another',\n",
              " 'any',\n",
              " 'anybody',\n",
              " 'anyhow',\n",
              " 'anymore',\n",
              " 'anyone',\n",
              " 'anything',\n",
              " 'anyway',\n",
              " 'anyways',\n",
              " 'anywhere',\n",
              " 'ao',\n",
              " 'apart',\n",
              " 'apparently',\n",
              " 'appear',\n",
              " 'appreciate',\n",
              " 'appropriate',\n",
              " 'approximately',\n",
              " 'aq',\n",
              " 'ar',\n",
              " 'are',\n",
              " 'area',\n",
              " 'areas',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'arent',\n",
              " 'arise',\n",
              " 'around',\n",
              " 'arpa',\n",
              " 'as',\n",
              " 'aside',\n",
              " 'ask',\n",
              " 'asked',\n",
              " 'asking',\n",
              " 'asks',\n",
              " 'associated',\n",
              " 'at',\n",
              " 'au',\n",
              " 'auth',\n",
              " 'available',\n",
              " 'aw',\n",
              " 'away',\n",
              " 'awfully',\n",
              " 'az',\n",
              " 'b',\n",
              " 'ba',\n",
              " 'back',\n",
              " 'backed',\n",
              " 'backing',\n",
              " 'backs',\n",
              " 'backward',\n",
              " 'backwards',\n",
              " 'bb',\n",
              " 'bd',\n",
              " 'be',\n",
              " 'became',\n",
              " 'because',\n",
              " 'become',\n",
              " 'becomes',\n",
              " 'becoming',\n",
              " 'been',\n",
              " 'before',\n",
              " 'beforehand',\n",
              " 'began',\n",
              " 'begin',\n",
              " 'beginning',\n",
              " 'beginnings',\n",
              " 'begins',\n",
              " 'behind',\n",
              " 'being',\n",
              " 'beings',\n",
              " 'believe',\n",
              " 'below',\n",
              " 'beside',\n",
              " 'besides',\n",
              " 'best',\n",
              " 'better',\n",
              " 'between',\n",
              " 'beyond',\n",
              " 'bf',\n",
              " 'bg',\n",
              " 'bh',\n",
              " 'bi',\n",
              " 'big',\n",
              " 'bill',\n",
              " 'billion',\n",
              " 'biol',\n",
              " 'bj',\n",
              " 'bm',\n",
              " 'bn',\n",
              " 'bo',\n",
              " 'both',\n",
              " 'bottom',\n",
              " 'br',\n",
              " 'brief',\n",
              " 'briefly',\n",
              " 'bs',\n",
              " 'bt',\n",
              " 'but',\n",
              " 'buy',\n",
              " 'bv',\n",
              " 'bw',\n",
              " 'by',\n",
              " 'bz',\n",
              " 'c',\n",
              " \"c'mon\",\n",
              " \"c's\",\n",
              " 'ca',\n",
              " 'call',\n",
              " 'came',\n",
              " 'can',\n",
              " \"can't\",\n",
              " 'cannot',\n",
              " 'cant',\n",
              " 'caption',\n",
              " 'case',\n",
              " 'cases',\n",
              " 'cause',\n",
              " 'causes',\n",
              " 'cc',\n",
              " 'cd',\n",
              " 'certain',\n",
              " 'certainly',\n",
              " 'cf',\n",
              " 'cg',\n",
              " 'ch',\n",
              " 'changes',\n",
              " 'ci',\n",
              " 'ck',\n",
              " 'cl',\n",
              " 'clear',\n",
              " 'clearly',\n",
              " 'click',\n",
              " 'cm',\n",
              " 'cmon',\n",
              " 'cn',\n",
              " 'co',\n",
              " 'co.',\n",
              " 'com',\n",
              " 'come',\n",
              " 'comes',\n",
              " 'computer',\n",
              " 'con',\n",
              " 'concerning',\n",
              " 'consequently',\n",
              " 'consider',\n",
              " 'considering',\n",
              " 'contain',\n",
              " 'containing',\n",
              " 'contains',\n",
              " 'copy',\n",
              " 'corresponding',\n",
              " 'could',\n",
              " \"could've\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'couldnt',\n",
              " 'course',\n",
              " 'cr',\n",
              " 'cry',\n",
              " 'cs',\n",
              " 'cu',\n",
              " 'currently',\n",
              " 'cv',\n",
              " 'cx',\n",
              " 'cy',\n",
              " 'cz',\n",
              " 'd',\n",
              " 'dare',\n",
              " \"daren't\",\n",
              " 'darent',\n",
              " 'date',\n",
              " 'de',\n",
              " 'dear',\n",
              " 'definitely',\n",
              " 'describe',\n",
              " 'described',\n",
              " 'despite',\n",
              " 'detail',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'didnt',\n",
              " 'differ',\n",
              " 'different',\n",
              " 'differently',\n",
              " 'directly',\n",
              " 'dj',\n",
              " 'dk',\n",
              " 'dm',\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doesnt',\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'done',\n",
              " 'dont',\n",
              " 'doubtful',\n",
              " 'down',\n",
              " 'downed',\n",
              " 'downing',\n",
              " 'downs',\n",
              " 'downwards',\n",
              " 'due',\n",
              " 'during',\n",
              " 'dz',\n",
              " 'e',\n",
              " 'each',\n",
              " 'early',\n",
              " 'ec',\n",
              " 'ed',\n",
              " 'edu',\n",
              " 'ee',\n",
              " 'effect',\n",
              " 'eg',\n",
              " 'eh',\n",
              " 'eight',\n",
              " 'eighty',\n",
              " 'either',\n",
              " 'eleven',\n",
              " 'else',\n",
              " 'elsewhere',\n",
              " 'empty',\n",
              " 'end',\n",
              " 'ended',\n",
              " 'ending',\n",
              " 'ends',\n",
              " 'enough',\n",
              " 'entirely',\n",
              " 'er',\n",
              " 'es',\n",
              " 'especially',\n",
              " 'et',\n",
              " 'et-al',\n",
              " 'etc',\n",
              " 'even',\n",
              " 'evenly',\n",
              " 'ever',\n",
              " 'evermore',\n",
              " 'every',\n",
              " 'everybody',\n",
              " 'everyone',\n",
              " 'everything',\n",
              " 'everywhere',\n",
              " 'ex',\n",
              " 'exactly',\n",
              " 'example',\n",
              " 'except',\n",
              " 'f',\n",
              " 'face',\n",
              " 'faces',\n",
              " 'fact',\n",
              " 'facts',\n",
              " 'fairly',\n",
              " 'far',\n",
              " 'farther',\n",
              " 'felt',\n",
              " 'few',\n",
              " 'fewer',\n",
              " 'ff',\n",
              " 'fi',\n",
              " 'fifteen',\n",
              " 'fifth',\n",
              " 'fifty',\n",
              " 'fify',\n",
              " 'fill',\n",
              " 'find',\n",
              " 'finds',\n",
              " 'fire',\n",
              " 'first',\n",
              " 'five',\n",
              " 'fix',\n",
              " 'fj',\n",
              " 'fk',\n",
              " 'fm',\n",
              " 'fo',\n",
              " 'followed',\n",
              " 'following',\n",
              " 'follows',\n",
              " 'for',\n",
              " 'forever',\n",
              " 'former',\n",
              " 'formerly',\n",
              " 'forth',\n",
              " 'forty',\n",
              " 'forward',\n",
              " 'found',\n",
              " 'four',\n",
              " 'fr',\n",
              " 'free',\n",
              " 'from',\n",
              " 'front',\n",
              " 'full',\n",
              " 'fully',\n",
              " 'further',\n",
              " 'furthered',\n",
              " 'furthering',\n",
              " 'furthermore',\n",
              " 'furthers',\n",
              " 'fx',\n",
              " 'g',\n",
              " 'ga',\n",
              " 'gave',\n",
              " 'gb',\n",
              " 'gd',\n",
              " 'ge',\n",
              " 'general',\n",
              " 'generally',\n",
              " 'get',\n",
              " 'gets',\n",
              " 'getting',\n",
              " 'gf',\n",
              " 'gg',\n",
              " 'gh',\n",
              " 'gi',\n",
              " 'give',\n",
              " 'given',\n",
              " 'gives',\n",
              " 'giving',\n",
              " 'gl',\n",
              " 'gm',\n",
              " 'gmt',\n",
              " 'gn',\n",
              " 'go',\n",
              " 'goes',\n",
              " 'going',\n",
              " 'gone',\n",
              " 'good',\n",
              " 'goods',\n",
              " 'got',\n",
              " 'gotten',\n",
              " 'gov',\n",
              " 'gp',\n",
              " 'gq',\n",
              " 'gr',\n",
              " 'great',\n",
              " 'greater',\n",
              " 'greatest',\n",
              " 'greetings',\n",
              " 'group',\n",
              " 'grouped',\n",
              " 'grouping',\n",
              " 'groups',\n",
              " 'gs',\n",
              " 'gt',\n",
              " 'gu',\n",
              " 'gw',\n",
              " 'gy',\n",
              " 'h',\n",
              " 'had',\n",
              " \"hadn't\",\n",
              " 'hadnt',\n",
              " 'half',\n",
              " 'happens',\n",
              " 'hardly',\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'hasnt',\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'havent',\n",
              " 'having',\n",
              " 'he',\n",
              " \"he'd\",\n",
              " \"he'll\",\n",
              " \"he's\",\n",
              " 'hed',\n",
              " 'hell',\n",
              " 'hello',\n",
              " 'help',\n",
              " 'hence',\n",
              " 'her',\n",
              " 'here',\n",
              " \"here's\",\n",
              " 'hereafter',\n",
              " 'hereby',\n",
              " 'herein',\n",
              " 'heres',\n",
              " 'hereupon',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'herseâ€',\n",
              " 'hes',\n",
              " 'hi',\n",
              " 'hid',\n",
              " 'high',\n",
              " 'higher',\n",
              " 'highest',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'himseâ€',\n",
              " 'his',\n",
              " 'hither',\n",
              " 'hk',\n",
              " 'hm',\n",
              " 'hn',\n",
              " 'home',\n",
              " 'homepage',\n",
              " 'hopefully',\n",
              " 'how',\n",
              " \"how'd\",\n",
              " \"how'll\",\n",
              " \"how's\",\n",
              " 'howbeit',\n",
              " 'however',\n",
              " 'hr',\n",
              " 'ht',\n",
              " 'htm',\n",
              " 'html',\n",
              " 'http',\n",
              " 'hu',\n",
              " 'hundred',\n",
              " 'i',\n",
              " \"i'd\",\n",
              " \"i'll\",\n",
              " \"i'm\",\n",
              " \"i've\",\n",
              " 'i.e.',\n",
              " 'id',\n",
              " 'ie',\n",
              " 'if',\n",
              " 'ignored',\n",
              " 'ii',\n",
              " 'il',\n",
              " 'ill',\n",
              " 'im',\n",
              " 'immediate',\n",
              " 'immediately',\n",
              " 'importance',\n",
              " 'important',\n",
              " 'in',\n",
              " 'inasmuch',\n",
              " 'inc',\n",
              " 'inc.',\n",
              " 'indeed',\n",
              " 'index',\n",
              " 'indicate',\n",
              " 'indicated',\n",
              " 'indicates',\n",
              " 'information',\n",
              " 'inner',\n",
              " 'inside',\n",
              " 'insofar',\n",
              " 'instead',\n",
              " 'int',\n",
              " 'interest',\n",
              " 'interested',\n",
              " 'interesting',\n",
              " 'interests',\n",
              " 'into',\n",
              " 'invention',\n",
              " 'inward',\n",
              " 'io',\n",
              " 'iq',\n",
              " 'ir',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'isnt',\n",
              " 'it',\n",
              " \"it'd\",\n",
              " \"it'll\",\n",
              " \"it's\",\n",
              " 'itd',\n",
              " 'itll',\n",
              " 'its',\n",
              " 'itself',\n",
              " 'itseâ€',\n",
              " 'ive',\n",
              " 'j',\n",
              " 'je',\n",
              " 'jm',\n",
              " 'jo',\n",
              " 'join',\n",
              " 'jp',\n",
              " 'just',\n",
              " 'k',\n",
              " 'ke',\n",
              " 'keep',\n",
              " 'keeps',\n",
              " 'kept',\n",
              " 'keys',\n",
              " 'kg',\n",
              " 'kh',\n",
              " 'ki',\n",
              " 'kind',\n",
              " 'km',\n",
              " 'kn',\n",
              " 'knew',\n",
              " 'know',\n",
              " 'known',\n",
              " 'knows',\n",
              " 'kp',\n",
              " 'kr',\n",
              " 'kw',\n",
              " 'ky',\n",
              " 'kz',\n",
              " 'l',\n",
              " 'la',\n",
              " 'large',\n",
              " 'largely',\n",
              " 'last',\n",
              " 'lately',\n",
              " 'later',\n",
              " 'latest',\n",
              " 'latter',\n",
              " 'latterly',\n",
              " 'lb',\n",
              " 'lc',\n",
              " 'least',\n",
              " 'length',\n",
              " 'less',\n",
              " 'lest',\n",
              " 'let',\n",
              " \"let's\",\n",
              " 'lets',\n",
              " 'li',\n",
              " 'like',\n",
              " 'liked',\n",
              " 'likely',\n",
              " 'likewise',\n",
              " 'line',\n",
              " 'little',\n",
              " 'lk',\n",
              " 'll',\n",
              " 'long',\n",
              " 'longer',\n",
              " 'longest',\n",
              " 'look',\n",
              " 'looking',\n",
              " 'looks',\n",
              " 'low',\n",
              " 'lower',\n",
              " 'lr',\n",
              " 'ls',\n",
              " 'lt',\n",
              " 'ltd',\n",
              " 'lu',\n",
              " 'lv',\n",
              " 'ly',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'made',\n",
              " 'mainly',\n",
              " 'make',\n",
              " 'makes',\n",
              " 'making',\n",
              " 'man',\n",
              " 'many',\n",
              " 'may',\n",
              " 'maybe',\n",
              " \"mayn't\",\n",
              " 'maynt',\n",
              " 'mc',\n",
              " 'md',\n",
              " 'me',\n",
              " 'mean',\n",
              " 'means',\n",
              " 'meantime',\n",
              " 'meanwhile',\n",
              " 'member',\n",
              " 'members',\n",
              " 'men',\n",
              " 'merely',\n",
              " 'mg',\n",
              " 'mh',\n",
              " 'microsoft',\n",
              " 'might',\n",
              " \"might've\",\n",
              " \"mightn't\",\n",
              " 'mightnt',\n",
              " 'mil',\n",
              " 'mill',\n",
              " 'million',\n",
              " 'mine',\n",
              " 'minus',\n",
              " 'miss',\n",
              " 'mk',\n",
              " 'ml',\n",
              " 'mm',\n",
              " 'mn',\n",
              " 'mo',\n",
              " 'more',\n",
              " 'moreover',\n",
              " 'most',\n",
              " 'mostly',\n",
              " 'move',\n",
              " 'mp',\n",
              " 'mq',\n",
              " 'mr',\n",
              " 'mrs',\n",
              " 'ms',\n",
              " 'msie',\n",
              " 'mt',\n",
              " 'mu',\n",
              " 'much',\n",
              " 'mug',\n",
              " 'must',\n",
              " \"must've\",\n",
              " \"mustn't\",\n",
              " 'mustnt',\n",
              " 'mv',\n",
              " 'mw',\n",
              " 'mx',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'myseâ€',\n",
              " 'mz',\n",
              " 'n',\n",
              " 'na',\n",
              " 'name',\n",
              " 'namely',\n",
              " 'nay',\n",
              " 'nc',\n",
              " 'nd',\n",
              " 'ne',\n",
              " 'near',\n",
              " 'nearly',\n",
              " 'necessarily',\n",
              " 'necessary',\n",
              " 'need',\n",
              " 'needed',\n",
              " 'needing',\n",
              " \"needn't\",\n",
              " 'neednt',\n",
              " 'needs',\n",
              " 'neither',\n",
              " 'net',\n",
              " 'netscape',\n",
              " 'never',\n",
              " 'neverf',\n",
              " 'neverless',\n",
              " 'nevertheless',\n",
              " 'new',\n",
              " 'newer',\n",
              " 'newest',\n",
              " 'next',\n",
              " 'nf',\n",
              " 'ng',\n",
              " 'ni',\n",
              " 'nine',\n",
              " 'ninety',\n",
              " 'nl',\n",
              " 'no',\n",
              " 'no-one',\n",
              " 'nobody',\n",
              " 'non',\n",
              " 'none',\n",
              " 'nonetheless',\n",
              " 'noone',\n",
              " 'nor',\n",
              " 'normally',\n",
              " 'nos',\n",
              " 'not',\n",
              " 'noted',\n",
              " 'nothing',\n",
              " 'notwithstanding',\n",
              " 'novel',\n",
              " 'now',\n",
              " 'nowhere',\n",
              " 'np',\n",
              " 'nr',\n",
              " 'nu',\n",
              " 'null',\n",
              " 'number',\n",
              " 'numbers',\n",
              " 'nz',\n",
              " 'o',\n",
              " 'obtain',\n",
              " 'obtained',\n",
              " 'obviously',\n",
              " 'of',\n",
              " 'off',\n",
              " 'often',\n",
              " 'oh',\n",
              " 'ok',\n",
              " 'okay',\n",
              " 'old',\n",
              " 'older',\n",
              " 'oldest',\n",
              " 'om',\n",
              " 'omitted',\n",
              " 'on',\n",
              " 'once',\n",
              " 'one',\n",
              " \"one's\",\n",
              " 'ones',\n",
              " 'only',\n",
              " 'onto',\n",
              " 'open',\n",
              " 'opened',\n",
              " 'opening',\n",
              " 'opens',\n",
              " 'opposite',\n",
              " 'or',\n",
              " 'ord',\n",
              " 'order',\n",
              " 'ordered',\n",
              " 'ordering',\n",
              " 'orders',\n",
              " 'org',\n",
              " 'other',\n",
              " 'others',\n",
              " 'otherwise',\n",
              " 'ought',\n",
              " \"oughtn't\",\n",
              " 'oughtnt',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'outside',\n",
              " 'over',\n",
              " 'overall',\n",
              " 'owing',\n",
              " 'own',\n",
              " 'p',\n",
              " 'pa',\n",
              " 'page',\n",
              " 'pages',\n",
              " 'part',\n",
              " 'parted',\n",
              " 'particular',\n",
              " 'particularly',\n",
              " 'parting',\n",
              " 'parts',\n",
              " 'past',\n",
              " 'pe',\n",
              " 'per',\n",
              " 'perhaps',\n",
              " 'pf',\n",
              " 'pg',\n",
              " 'ph',\n",
              " 'pk',\n",
              " 'pl',\n",
              " 'place',\n",
              " 'placed',\n",
              " 'places',\n",
              " 'please',\n",
              " 'plus',\n",
              " 'pm',\n",
              " 'pmid',\n",
              " 'pn',\n",
              " 'point',\n",
              " 'pointed',\n",
              " 'pointing',\n",
              " 'points',\n",
              " 'poorly',\n",
              " 'possible',\n",
              " 'possibly',\n",
              " 'potentially',\n",
              " 'pp',\n",
              " 'pr',\n",
              " 'predominantly',\n",
              " 'present',\n",
              " 'presented',\n",
              " 'presenting',\n",
              " 'presents',\n",
              " 'presumably',\n",
              " 'previously',\n",
              " 'primarily',\n",
              " 'probably',\n",
              " 'problem',\n",
              " 'problems',\n",
              " 'promptly',\n",
              " 'proud',\n",
              " 'provided',\n",
              " 'provides',\n",
              " 'pt',\n",
              " 'put',\n",
              " 'puts',\n",
              " 'pw',\n",
              " 'py',\n",
              " 'q',\n",
              " 'qa',\n",
              " 'que',\n",
              " 'quickly',\n",
              " 'quite',\n",
              " 'qv',\n",
              " 'r',\n",
              " 'ran',\n",
              " 'rather',\n",
              " 'rd',\n",
              " 're',\n",
              " 'readily',\n",
              " 'really',\n",
              " 'reasonably',\n",
              " 'recent',\n",
              " 'recently',\n",
              " 'ref',\n",
              " 'refs',\n",
              " 'regarding',\n",
              " 'regardless',\n",
              " 'regards',\n",
              " 'related',\n",
              " 'relatively',\n",
              " 'research',\n",
              " 'reserved',\n",
              " 'respectively',\n",
              " 'resulted',\n",
              " 'resulting',\n",
              " 'results',\n",
              " 'right',\n",
              " 'ring',\n",
              " 'ro',\n",
              " 'room',\n",
              " 'rooms',\n",
              " 'round',\n",
              " 'ru',\n",
              " 'run',\n",
              " 'rw',\n",
              " 's',\n",
              " 'sa',\n",
              " 'said',\n",
              " 'same',\n",
              " 'saw',\n",
              " 'say',\n",
              " 'saying',\n",
              " 'says',\n",
              " 'sb',\n",
              " 'sc',\n",
              " 'sd',\n",
              " 'se',\n",
              " 'sec',\n",
              " 'second',\n",
              " 'secondly',\n",
              " 'seconds',\n",
              " 'section',\n",
              " 'see',\n",
              " 'seeing',\n",
              " 'seem',\n",
              " 'seemed',\n",
              " 'seeming',\n",
              " 'seems',\n",
              " 'seen',\n",
              " 'sees',\n",
              " 'self',\n",
              " 'selves',\n",
              " 'sensible',\n",
              " 'sent',\n",
              " 'serious',\n",
              " 'seriously',\n",
              " 'seven',\n",
              " 'seventy',\n",
              " 'several',\n",
              " 'sg',\n",
              " 'sh',\n",
              " 'shall',\n",
              " \"shan't\",\n",
              " 'shant',\n",
              " 'she',\n",
              " \"she'd\",\n",
              " \"she'll\",\n",
              " \"she's\",\n",
              " 'shed',\n",
              " 'shell',\n",
              " 'shes',\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'shouldnt',\n",
              " 'show',\n",
              " 'showed',\n",
              " 'showing',\n",
              " 'shown',\n",
              " 'showns',\n",
              " 'shows',\n",
              " 'si',\n",
              " 'side',\n",
              " 'sides',\n",
              " 'significant',\n",
              " 'significantly',\n",
              " 'similar',\n",
              " 'similarly',\n",
              " 'since',\n",
              " 'sincere',\n",
              " 'site',\n",
              " 'six',\n",
              " 'sixty',\n",
              " 'sj',\n",
              " 'sk',\n",
              " 'sl',\n",
              " 'slightly',\n",
              " 'sm',\n",
              " 'small',\n",
              " 'smaller',\n",
              " 'smallest',\n",
              " 'sn',\n",
              " 'so',\n",
              " 'some',\n",
              " 'somebody',\n",
              " 'someday',\n",
              " 'somehow',\n",
              " 'someone',\n",
              " 'somethan',\n",
              " 'something',\n",
              " 'sometime',\n",
              " 'sometimes',\n",
              " 'somewhat',\n",
              " 'somewhere',\n",
              " 'soon',\n",
              " 'sorry',\n",
              " 'specifically',\n",
              " 'specified',\n",
              " 'specify',\n",
              " 'specifying',\n",
              " 'sr',\n",
              " 'st',\n",
              " 'state',\n",
              " 'states',\n",
              " 'still',\n",
              " 'stop',\n",
              " 'strongly',\n",
              " 'su',\n",
              " 'sub',\n",
              " 'substantially',\n",
              " 'successfully',\n",
              " 'such',\n",
              " 'sufficiently',\n",
              " 'suggest',\n",
              " 'sup',\n",
              " 'sure',\n",
              " 'sv',\n",
              " 'sy',\n",
              " 'system',\n",
              " 'sz',\n",
              " 't',\n",
              " \"t's\",\n",
              " 'take',\n",
              " 'taken',\n",
              " 'taking',\n",
              " 'tc',\n",
              " 'td',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXUXOkhgnRol"
      },
      "source": [
        "en_sw = stopwords.stopwords('en')\n",
        "\n",
        "def word_counts(x, pipeline=en):\n",
        "  doc = pipeline(x)\n",
        "  count = 0\n",
        "  for sentence in doc.sentences:\n",
        "    for token in sentence.tokens:\n",
        "        if token.text.lower() not in en_sw:\n",
        "          count += 1\n",
        "  return count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Trbxkc1B4sNP"
      },
      "source": [
        "train['Words'] = train['Message'].apply(word_counts)\n",
        "test['Words'] = test['Message'].apply(word_counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "uXxl8S1vCGi3",
        "outputId": "55b7ebc1-461c-4134-c4ca-d5edb7aabc7f"
      },
      "source": [
        "x_train = train[['Length', 'Punctuation', 'Capitals', 'Words']]\n",
        "y_train = train[['Spam']]\n",
        "\n",
        "x_test = test[['Length', 'Punctuation', 'Capitals' , 'Words']]\n",
        "y_test = test[['Spam']]\n",
        "\n",
        "model = make_model(input_dims=4)\n",
        "#model = make_model(input_dims=3)\n",
        "\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.5050 - accuracy: 0.8778\n",
            "Epoch 2/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.2953 - accuracy: 0.9159\n",
            "Epoch 3/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.2448 - accuracy: 0.9253\n",
            "Epoch 4/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.2150 - accuracy: 0.9312\n",
            "Epoch 5/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.2045 - accuracy: 0.9316\n",
            "Epoch 6/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.1978 - accuracy: 0.9377\n",
            "Epoch 7/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.1936 - accuracy: 0.9379\n",
            "Epoch 8/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.1891 - accuracy: 0.9363\n",
            "Epoch 9/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.1902 - accuracy: 0.9356\n",
            "Epoch 10/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.2000 - accuracy: 0.9336\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3832aa7d30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4-0huTw5YFe"
      },
      "source": [
        "## POS Based Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "3bSXLgr6iiB0",
        "outputId": "5dd25cbe-3fea-4ba9-f3aa-0598a4558e1b"
      },
      "source": [
        "en = stanza.Pipeline(lang='en')\n",
        "\n",
        "txt = \"Yo you around? A friend of mine's lookin.\"\n",
        "pos = en(txt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-14 04:51:48 INFO: Loading these models for language: en (English):\n",
            "=========================\n",
            "| Processor | Package   |\n",
            "-------------------------\n",
            "| tokenize  | ewt       |\n",
            "| pos       | ewt       |\n",
            "| lemma     | ewt       |\n",
            "| depparse  | ewt       |\n",
            "| sentiment | sstplus   |\n",
            "| ner       | ontonotes |\n",
            "=========================\n",
            "\n",
            "2020-10-14 04:51:48 INFO: Use device: gpu\n",
            "2020-10-14 04:51:48 INFO: Loading: tokenize\n",
            "2020-10-14 04:51:48 INFO: Loading: pos\n",
            "2020-10-14 04:51:49 INFO: Loading: lemma\n",
            "2020-10-14 04:51:49 INFO: Loading: depparse\n",
            "2020-10-14 04:51:50 INFO: Loading: sentiment\n",
            "2020-10-14 04:51:51 INFO: Loading: ner\n",
            "2020-10-14 04:51:51 INFO: Done loading processors!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dD9BUu6gkNi_"
      },
      "source": [
        "def print_pos(doc):\n",
        "    text = \"\"\n",
        "    for sentence in doc.sentences:\n",
        "        for token in sentence.tokens:\n",
        "            text += token.words[0].text + \"/\" + \\\n",
        "                    token.words[0].upos + \" \"\n",
        "        text += \"\\n\"\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "vTLrN-BgkuWV",
        "outputId": "1799c33f-1820-448e-b074-b3f4732119a4"
      },
      "source": [
        "print(print_pos(pos))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Yo/PRON you/PRON around/ADV ?/PUNCT \n",
            "A/DET friend/NOUN of/ADP mine/PRON 's/PART lookin/NOUN ./PUNCT \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1i_BcxgK5cs6"
      },
      "source": [
        "en_sw = stopwords.stopwords('en')\n",
        "\n",
        "def word_counts_v3(x, pipeline=en):\n",
        "  doc = pipeline(x)\n",
        "  count = 0\n",
        "  for sentence in doc.sentences:\n",
        "    for token in sentence.tokens:\n",
        "        if token.text.lower() not in en_sw and \\\n",
        "        token.words[0].upos not in ['PUNCT', 'SYM']:\n",
        "          count += 1\n",
        "  return count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "j4ddhhiY9FgG",
        "outputId": "acace7c9-9dc7-48bd-df18-d057340c170c"
      },
      "source": [
        "print(word_counts(txt), word_counts_v3(txt))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "99upx22tCHgz",
        "outputId": "f523ebfe-d63c-4a20-e0d8-75e8f41cc5b7"
      },
      "source": [
        "train['Test'] = 0\n",
        "train.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Spam</th>\n",
              "      <th>Capitals</th>\n",
              "      <th>Punctuation</th>\n",
              "      <th>Length</th>\n",
              "      <th>Words</th>\n",
              "      <th>Test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4459.000000</td>\n",
              "      <td>4459.000000</td>\n",
              "      <td>4459.000000</td>\n",
              "      <td>4459.000000</td>\n",
              "      <td>4459.000000</td>\n",
              "      <td>4459.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.132765</td>\n",
              "      <td>5.519399</td>\n",
              "      <td>18.886522</td>\n",
              "      <td>80.316439</td>\n",
              "      <td>9.326979</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.339359</td>\n",
              "      <td>11.405424</td>\n",
              "      <td>14.602023</td>\n",
              "      <td>59.346407</td>\n",
              "      <td>8.016488</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>61.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>27.000000</td>\n",
              "      <td>122.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>129.000000</td>\n",
              "      <td>253.000000</td>\n",
              "      <td>910.000000</td>\n",
              "      <td>147.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Spam     Capitals  Punctuation       Length        Words    Test\n",
              "count  4459.000000  4459.000000  4459.000000  4459.000000  4459.000000  4459.0\n",
              "mean      0.132765     5.519399    18.886522    80.316439     9.326979     0.0\n",
              "std       0.339359    11.405424    14.602023    59.346407     8.016488     0.0\n",
              "min       0.000000     0.000000     0.000000     2.000000     0.000000     0.0\n",
              "25%       0.000000     1.000000     8.000000    35.000000     4.000000     0.0\n",
              "50%       0.000000     2.000000    15.000000    61.000000     7.000000     0.0\n",
              "75%       0.000000     4.000000    27.000000   122.000000    13.000000     0.0\n",
              "max       1.000000   129.000000   253.000000   910.000000   147.000000     0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGkdUgEdEDcB"
      },
      "source": [
        "def word_counts_v3(x, pipeline=en):\n",
        "  doc = pipeline(x)\n",
        "  totals = 0.\n",
        "  count = 0.\n",
        "  non_word = 0.\n",
        "  for sentence in doc.sentences:\n",
        "    totals += len(sentence.tokens)  # (1)\n",
        "    for token in sentence.tokens:\n",
        "        if token.text.lower() not in en_sw:\n",
        "          if token.words[0].upos not in ['PUNCT', 'SYM']:\n",
        "            count += 1.\n",
        "          else:\n",
        "            non_word += 1.\n",
        "  non_word = non_word / totals\n",
        "  return pd.Series([count, non_word], index=['Words_NoPunct', 'Punct'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "Y6LWE7QWIuyu",
        "outputId": "aa347b17-f227-461f-a30f-04b588027483"
      },
      "source": [
        "x = train[:10]\n",
        "x.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Spam</th>\n",
              "      <th>Capitals</th>\n",
              "      <th>Punctuation</th>\n",
              "      <th>Length</th>\n",
              "      <th>Words</th>\n",
              "      <th>Test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>10.0</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.00000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.0</td>\n",
              "      <td>14.400000</td>\n",
              "      <td>18.300000</td>\n",
              "      <td>72.70000</td>\n",
              "      <td>8.600000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.0</td>\n",
              "      <td>32.948445</td>\n",
              "      <td>14.772723</td>\n",
              "      <td>50.36103</td>\n",
              "      <td>10.068653</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>23.00000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>7.250000</td>\n",
              "      <td>37.75000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>57.00000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>23.750000</td>\n",
              "      <td>88.00000</td>\n",
              "      <td>10.750000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.0</td>\n",
              "      <td>107.000000</td>\n",
              "      <td>48.000000</td>\n",
              "      <td>161.00000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Spam    Capitals  Punctuation     Length      Words  Test\n",
              "count  10.0   10.000000    10.000000   10.00000  10.000000  10.0\n",
              "mean    0.0   14.400000    18.300000   72.70000   8.600000   0.0\n",
              "std     0.0   32.948445    14.772723   50.36103  10.068653   0.0\n",
              "min     0.0    1.000000     4.000000   23.00000   2.000000   0.0\n",
              "25%     0.0    1.000000     7.250000   37.75000   3.000000   0.0\n",
              "50%     0.0    1.500000    13.000000   57.00000   4.000000   0.0\n",
              "75%     0.0    9.000000    23.750000   88.00000  10.750000   0.0\n",
              "max     0.0  107.000000    48.000000  161.00000  35.000000   0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "97Y6_7E8KJnQ",
        "outputId": "57f1ff3d-3fd1-408b-85c8-094570a75804"
      },
      "source": [
        "train_tmp = train['Message'].apply(word_counts_v3)\n",
        "train = pd.concat([train, train_tmp], axis=1)\n",
        "train.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Spam</th>\n",
              "      <th>Capitals</th>\n",
              "      <th>Punctuation</th>\n",
              "      <th>Length</th>\n",
              "      <th>Words</th>\n",
              "      <th>Test</th>\n",
              "      <th>Words_NoPunct</th>\n",
              "      <th>Punct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4459.000000</td>\n",
              "      <td>4459.000000</td>\n",
              "      <td>4459.000000</td>\n",
              "      <td>4459.000000</td>\n",
              "      <td>4459.000000</td>\n",
              "      <td>4459.0</td>\n",
              "      <td>4459.000000</td>\n",
              "      <td>4459.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.132765</td>\n",
              "      <td>5.519399</td>\n",
              "      <td>18.886522</td>\n",
              "      <td>80.316439</td>\n",
              "      <td>9.326979</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.535995</td>\n",
              "      <td>0.147763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.339359</td>\n",
              "      <td>11.405424</td>\n",
              "      <td>14.602023</td>\n",
              "      <td>59.346407</td>\n",
              "      <td>8.016488</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.679984</td>\n",
              "      <td>0.094337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>61.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.142857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>27.000000</td>\n",
              "      <td>122.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>129.000000</td>\n",
              "      <td>253.000000</td>\n",
              "      <td>910.000000</td>\n",
              "      <td>147.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>54.000000</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Spam     Capitals  ...  Words_NoPunct        Punct\n",
              "count  4459.000000  4459.000000  ...    4459.000000  4459.000000\n",
              "mean      0.132765     5.519399  ...       6.535995     0.147763\n",
              "std       0.339359    11.405424  ...       5.679984     0.094337\n",
              "min       0.000000     0.000000  ...       0.000000     0.000000\n",
              "25%       0.000000     1.000000  ...       3.000000     0.090909\n",
              "50%       0.000000     2.000000  ...       5.000000     0.142857\n",
              "75%       0.000000     4.000000  ...       9.000000     0.200000\n",
              "max       1.000000   129.000000  ...      54.000000     0.666667\n",
              "\n",
              "[8 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "0WUdTWXvWbvc",
        "outputId": "70483b7a-ba7e-4530-b5e8-a40142293d03"
      },
      "source": [
        "test_tmp = test['Message'].apply(word_counts_v3)\n",
        "test = pd.concat([test, test_tmp], axis=1)\n",
        "test.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Spam</th>\n",
              "      <th>Capitals</th>\n",
              "      <th>Punctuation</th>\n",
              "      <th>Length</th>\n",
              "      <th>Words</th>\n",
              "      <th>Words_NoPunct</th>\n",
              "      <th>Punct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1115.000000</td>\n",
              "      <td>1115.000000</td>\n",
              "      <td>1115.000000</td>\n",
              "      <td>1115.000000</td>\n",
              "      <td>1115.000000</td>\n",
              "      <td>1115.000000</td>\n",
              "      <td>1115.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.139013</td>\n",
              "      <td>6.030493</td>\n",
              "      <td>19.166816</td>\n",
              "      <td>80.951570</td>\n",
              "      <td>9.623318</td>\n",
              "      <td>6.700448</td>\n",
              "      <td>0.152936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.346116</td>\n",
              "      <td>12.731059</td>\n",
              "      <td>15.694599</td>\n",
              "      <td>61.807655</td>\n",
              "      <td>8.303803</td>\n",
              "      <td>5.887786</td>\n",
              "      <td>0.101909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>36.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.096774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>61.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.142857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>123.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>127.000000</td>\n",
              "      <td>195.000000</td>\n",
              "      <td>790.000000</td>\n",
              "      <td>83.000000</td>\n",
              "      <td>45.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Spam     Capitals  ...  Words_NoPunct        Punct\n",
              "count  1115.000000  1115.000000  ...    1115.000000  1115.000000\n",
              "mean      0.139013     6.030493  ...       6.700448     0.152936\n",
              "std       0.346116    12.731059  ...       5.887786     0.101909\n",
              "min       0.000000     0.000000  ...       0.000000     0.000000\n",
              "25%       0.000000     1.000000  ...       3.000000     0.096774\n",
              "50%       0.000000     2.000000  ...       4.000000     0.142857\n",
              "75%       0.000000     4.000000  ...      10.000000     0.200000\n",
              "max       1.000000   127.000000  ...      45.000000     1.000000\n",
              "\n",
              "[8 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "tMv1q8_mKdjI",
        "outputId": "c4d4bff6-d9e3-435c-b0d4-73573c3b6791"
      },
      "source": [
        "z = pd.concat([x, train_tmp], axis=1)\n",
        "z.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Spam</th>\n",
              "      <th>Capitals</th>\n",
              "      <th>Punctuation</th>\n",
              "      <th>Length</th>\n",
              "      <th>Words</th>\n",
              "      <th>Test</th>\n",
              "      <th>Words_NoPunct</th>\n",
              "      <th>Punct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>10.0</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.00000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.0</td>\n",
              "      <td>4459.000000</td>\n",
              "      <td>4459.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.0</td>\n",
              "      <td>14.400000</td>\n",
              "      <td>18.300000</td>\n",
              "      <td>72.70000</td>\n",
              "      <td>8.600000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.535995</td>\n",
              "      <td>0.147763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.0</td>\n",
              "      <td>32.948445</td>\n",
              "      <td>14.772723</td>\n",
              "      <td>50.36103</td>\n",
              "      <td>10.068653</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.679984</td>\n",
              "      <td>0.094337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>23.00000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>7.250000</td>\n",
              "      <td>37.75000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>57.00000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.142857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>23.750000</td>\n",
              "      <td>88.00000</td>\n",
              "      <td>10.750000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.0</td>\n",
              "      <td>107.000000</td>\n",
              "      <td>48.000000</td>\n",
              "      <td>161.00000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>54.000000</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Spam    Capitals  Punctuation  ...  Test  Words_NoPunct        Punct\n",
              "count  10.0   10.000000    10.000000  ...  10.0    4459.000000  4459.000000\n",
              "mean    0.0   14.400000    18.300000  ...   0.0       6.535995     0.147763\n",
              "std     0.0   32.948445    14.772723  ...   0.0       5.679984     0.094337\n",
              "min     0.0    1.000000     4.000000  ...   0.0       0.000000     0.000000\n",
              "25%     0.0    1.000000     7.250000  ...   0.0       3.000000     0.090909\n",
              "50%     0.0    1.500000    13.000000  ...   0.0       5.000000     0.142857\n",
              "75%     0.0    9.000000    23.750000  ...   0.0       9.000000     0.200000\n",
              "max     0.0  107.000000    48.000000  ...   0.0      54.000000     0.666667\n",
              "\n",
              "[8 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "8jkFZUJcPHA3",
        "outputId": "1aba40b1-768f-42e7-cb0f-b6d803503023"
      },
      "source": [
        "z.loc[z['Spam']==0].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Spam</th>\n",
              "      <th>Capitals</th>\n",
              "      <th>Punctuation</th>\n",
              "      <th>Length</th>\n",
              "      <th>Words</th>\n",
              "      <th>Test</th>\n",
              "      <th>Words_NoPunct</th>\n",
              "      <th>Punct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>10.0</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.00000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.0</td>\n",
              "      <td>14.400000</td>\n",
              "      <td>18.300000</td>\n",
              "      <td>72.70000</td>\n",
              "      <td>8.600000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.500000</td>\n",
              "      <td>0.151479</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.0</td>\n",
              "      <td>32.948445</td>\n",
              "      <td>14.772723</td>\n",
              "      <td>50.36103</td>\n",
              "      <td>10.068653</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.412452</td>\n",
              "      <td>0.063396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>23.00000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>7.250000</td>\n",
              "      <td>37.75000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.130721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>57.00000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>23.750000</td>\n",
              "      <td>88.00000</td>\n",
              "      <td>10.750000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.750000</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.0</td>\n",
              "      <td>107.000000</td>\n",
              "      <td>48.000000</td>\n",
              "      <td>161.00000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.208333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Spam    Capitals  Punctuation  ...  Test  Words_NoPunct      Punct\n",
              "count  10.0   10.000000    10.000000  ...  10.0      10.000000  10.000000\n",
              "mean    0.0   14.400000    18.300000  ...   0.0       5.500000   0.151479\n",
              "std     0.0   32.948445    14.772723  ...   0.0       7.412452   0.063396\n",
              "min     0.0    1.000000     4.000000  ...   0.0       1.000000   0.000000\n",
              "25%     0.0    1.000000     7.250000  ...   0.0       2.000000   0.130721\n",
              "50%     0.0    1.500000    13.000000  ...   0.0       2.000000   0.166667\n",
              "75%     0.0    9.000000    23.750000  ...   0.0       6.750000   0.200000\n",
              "max     0.0  107.000000    48.000000  ...   0.0      25.000000   0.208333\n",
              "\n",
              "[8 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "l86GIgYvP9Mc",
        "outputId": "42f8de52-315c-4a29-ac78-c4d7250fbb0a"
      },
      "source": [
        "z.loc[z['Spam']==1].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Spam</th>\n",
              "      <th>Capitals</th>\n",
              "      <th>Punctuation</th>\n",
              "      <th>Length</th>\n",
              "      <th>Words</th>\n",
              "      <th>Test</th>\n",
              "      <th>Words_NoPunct</th>\n",
              "      <th>Punct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Spam  Capitals  Punctuation  Length  Words  Test  Words_NoPunct  Punct\n",
              "count   0.0       0.0          0.0     0.0    0.0   0.0            0.0    0.0\n",
              "mean    NaN       NaN          NaN     NaN    NaN   NaN            NaN    NaN\n",
              "std     NaN       NaN          NaN     NaN    NaN   NaN            NaN    NaN\n",
              "min     NaN       NaN          NaN     NaN    NaN   NaN            NaN    NaN\n",
              "25%     NaN       NaN          NaN     NaN    NaN   NaN            NaN    NaN\n",
              "50%     NaN       NaN          NaN     NaN    NaN   NaN            NaN    NaN\n",
              "75%     NaN       NaN          NaN     NaN    NaN   NaN            NaN    NaN\n",
              "max     NaN       NaN          NaN     NaN    NaN   NaN            NaN    NaN"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-PdmMu_R01u"
      },
      "source": [
        "aa = [word_counts_v3(y) for y in x['Message']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "gBxbhHD_SMPH",
        "outputId": "056a07c1-7490-4e5b-e6af-b4bec050b1f3"
      },
      "source": [
        "ab = pd.DataFrame(aa)\n",
        "ab.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Words_NoPunct</th>\n",
              "      <th>Punct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>5.500000</td>\n",
              "      <td>0.151479</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>7.412452</td>\n",
              "      <td>0.063396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.130721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>6.750000</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.208333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Words_NoPunct      Punct\n",
              "count      10.000000  10.000000\n",
              "mean        5.500000   0.151479\n",
              "std         7.412452   0.063396\n",
              "min         1.000000   0.000000\n",
              "25%         2.000000   0.130721\n",
              "50%         2.000000   0.166667\n",
              "75%         6.750000   0.200000\n",
              "max        25.000000   0.208333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__TVLnoy-nre"
      },
      "source": [
        "# Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbXe_2yL-qb4"
      },
      "source": [
        "\n",
        "text = \"Stemming is aimed at reducing vocabulary and aid un-derstanding of\" +\\\n",
        "       \" morphological processes. This helps people un-derstand the\" +\\\n",
        "       \" morphology of words and reduce size of corpus.\"\n",
        "\n",
        "lemma = en(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "s8VfJ07pDEgN",
        "outputId": "9a5b8b26-de82-416d-ea41-6f7bcec884b7"
      },
      "source": [
        "lemmas = \"\"\n",
        "for sentence in lemma.sentences:\n",
        "        for token in sentence.tokens:\n",
        "            lemmas += token.words[0].lemma +\"/\" + \\\n",
        "                    token.words[0].upos + \" \"\n",
        "        lemmas += \"\\n\"\n",
        "\n",
        "print(lemmas)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "stemming/NOUN be/AUX aim/VERB at/SCONJ reduce/VERB vocabulary/NOUN and/CCONJ aid/NOUN un/NOUN -/PUNCT derstanding/NOUN of/ADP morphological/ADJ process/NOUN ./PUNCT \n",
            "this/PRON help/VERB people/NOUN un/NOUN -/PUNCT derstand/VERB the/DET morphology/NOUN of/ADP word/NOUN and/CCONJ reduce/VERB size/NOUN of/ADP corpus/NOUN ./PUNCT \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50W5rWfeOoR3"
      },
      "source": [
        "# TF-IDF Based Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "fy7RPjvfDRXz",
        "outputId": "5d0b1786-c4a9-4981-f69a-e6a11113a008"
      },
      "source": [
        "# if not installed already\n",
        "!pip install sklearn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cco-INbyhouu"
      },
      "source": [
        "corpus = [\n",
        "          \"I like fruits. Fruits like bananas\",\n",
        "          \"I love bananas but eat an apple\",\n",
        "          \"An apple a day keeps the doctor away\"\n",
        "]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbftF_xOEN9O"
      },
      "source": [
        "## Count Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "clVOFDb1ERTL",
        "outputId": "1325f9bf-f9de-4ec0-c1a9-4507c899b88e"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "vectorizer.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['an',\n",
              " 'apple',\n",
              " 'away',\n",
              " 'bananas',\n",
              " 'but',\n",
              " 'day',\n",
              " 'doctor',\n",
              " 'eat',\n",
              " 'fruits',\n",
              " 'keeps',\n",
              " 'like',\n",
              " 'love',\n",
              " 'the']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "KM7jlBZdFmiD",
        "outputId": "b1746158-25ba-4585-c995-9fd35d4c758c"
      },
      "source": [
        "X.toarray()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 2, 0, 0],\n",
              "       [1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0],\n",
              "       [1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "_pbT9sH-GwvM",
        "outputId": "56e3008d-175d-4d69-9839-ff204c7a1cdc"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "cosine_similarity(X.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.        , 0.13608276, 0.        ],\n",
              "       [0.13608276, 1.        , 0.3086067 ],\n",
              "       [0.        , 0.3086067 , 1.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "Fe6qPCQ4Gxb0",
        "outputId": "fea17707-d1b4-4a02-a986-f4ed045f4125"
      },
      "source": [
        "query = vectorizer.transform([\"apple and bananas\"])\n",
        "\n",
        "cosine_similarity(X, query)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.23570226],\n",
              "       [0.57735027],\n",
              "       [0.26726124]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7w2zVnOnER_3"
      },
      "source": [
        "## TF-IDF Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "xKyNZgATHmhM",
        "outputId": "f7f0e74c-81fc-48e2-ef87-8046eab50f76"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "transformer = TfidfTransformer(smooth_idf=False)\n",
        "tfidf = transformer.fit_transform(X.toarray())\n",
        "\n",
        "pd.DataFrame(tfidf.toarray(), \n",
        "             columns=vectorizer.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>an</th>\n",
              "      <th>apple</th>\n",
              "      <th>away</th>\n",
              "      <th>bananas</th>\n",
              "      <th>but</th>\n",
              "      <th>day</th>\n",
              "      <th>doctor</th>\n",
              "      <th>eat</th>\n",
              "      <th>fruits</th>\n",
              "      <th>keeps</th>\n",
              "      <th>like</th>\n",
              "      <th>love</th>\n",
              "      <th>the</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.230408</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.688081</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.688081</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.321267</td>\n",
              "      <td>0.321267</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.321267</td>\n",
              "      <td>0.479709</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.479709</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.479709</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.275785</td>\n",
              "      <td>0.275785</td>\n",
              "      <td>0.411797</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.411797</td>\n",
              "      <td>0.411797</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.411797</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.411797</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         an     apple      away  ...      like      love       the\n",
              "0  0.000000  0.000000  0.000000  ...  0.688081  0.000000  0.000000\n",
              "1  0.321267  0.321267  0.000000  ...  0.000000  0.479709  0.000000\n",
              "2  0.275785  0.275785  0.411797  ...  0.000000  0.000000  0.411797\n",
              "\n",
              "[3 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rifoynAZvO9H"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "tfidf = TfidfVectorizer(binary=True)\n",
        "X = tfidf.fit_transform(train['Message']).astype('float32')\n",
        "X_test = tfidf.transform(test['Message']).astype('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "bDnG2OPHwD7U",
        "outputId": "39c46fa8-dc12-4391-8f39-7c9d9cc2a37f"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4459, 7741)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "aGujuysLwKJe",
        "outputId": "fe7fae62-64b9-480d-e596-02588b22ce61"
      },
      "source": [
        "from keras.utils import np_utils\n",
        "\n",
        "_, cols = X.shape\n",
        "model2 = make_model(cols)  # to match tf-idf dimensions\n",
        "lb = LabelEncoder()\n",
        "y = lb.fit_transform(y_train)\n",
        "dummy_y_train = np_utils.to_categorical(y)\n",
        "model2.fit(X.toarray(), y_train, epochs=10, batch_size=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:251: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.3636 - accuracy: 0.8874\n",
            "Epoch 2/10\n",
            "446/446 [==============================] - 2s 4ms/step - loss: 0.1043 - accuracy: 0.9751\n",
            "Epoch 3/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.0477 - accuracy: 0.9899\n",
            "Epoch 4/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.0276 - accuracy: 0.9939\n",
            "Epoch 5/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.0174 - accuracy: 0.9966\n",
            "Epoch 6/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.0115 - accuracy: 0.9987\n",
            "Epoch 7/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.0079 - accuracy: 0.9991\n",
            "Epoch 8/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.0055 - accuracy: 0.9991\n",
            "Epoch 9/10\n",
            "446/446 [==============================] - 2s 3ms/step - loss: 0.0039 - accuracy: 0.9996\n",
            "Epoch 10/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.0029 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f37d1fb7828>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "xXYe6f8my21Y",
        "outputId": "d05d2f28-b07d-4292-b977-2e2e03985195"
      },
      "source": [
        "model2.evaluate(X_test.toarray(), y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0577 - accuracy: 0.9839\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.05765564367175102, 0.9838564991950989]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "ptf40YDjh71c",
        "outputId": "1139c8e5-4e91-42a7-915d-5303b607db77"
      },
      "source": [
        "train.loc[train.Spam == 1].describe() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Spam</th>\n",
              "      <th>Capitals</th>\n",
              "      <th>Punctuation</th>\n",
              "      <th>Length</th>\n",
              "      <th>Words</th>\n",
              "      <th>Test</th>\n",
              "      <th>Words_NoPunct</th>\n",
              "      <th>Punct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>592.0</td>\n",
              "      <td>592.000000</td>\n",
              "      <td>592.000000</td>\n",
              "      <td>592.000000</td>\n",
              "      <td>592.000000</td>\n",
              "      <td>592.0</td>\n",
              "      <td>592.000000</td>\n",
              "      <td>592.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.0</td>\n",
              "      <td>15.320946</td>\n",
              "      <td>29.086149</td>\n",
              "      <td>138.856419</td>\n",
              "      <td>18.469595</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14.250000</td>\n",
              "      <td>0.138386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.0</td>\n",
              "      <td>11.635105</td>\n",
              "      <td>7.083572</td>\n",
              "      <td>28.079980</td>\n",
              "      <td>6.085607</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.701046</td>\n",
              "      <td>0.064732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>132.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>0.096774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>149.000000</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>0.137931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>34.000000</td>\n",
              "      <td>157.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>0.176471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.0</td>\n",
              "      <td>128.000000</td>\n",
              "      <td>49.000000</td>\n",
              "      <td>197.000000</td>\n",
              "      <td>33.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.000000</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Spam    Capitals  Punctuation  ...   Test  Words_NoPunct       Punct\n",
              "count  592.0  592.000000   592.000000  ...  592.0     592.000000  592.000000\n",
              "mean     1.0   15.320946    29.086149  ...    0.0      14.250000    0.138386\n",
              "std      0.0   11.635105     7.083572  ...    0.0       4.701046    0.064732\n",
              "min      1.0    0.000000     2.000000  ...    0.0       2.000000    0.000000\n",
              "25%      1.0    7.000000    26.000000  ...    0.0      11.000000    0.096774\n",
              "50%      1.0   14.000000    30.000000  ...    0.0      14.000000    0.137931\n",
              "75%      1.0   21.000000    34.000000  ...    0.0      18.000000    0.176471\n",
              "max      1.0  128.000000    49.000000  ...    0.0      27.000000    0.333333\n",
              "\n",
              "[8 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMMB-9H3IEwm"
      },
      "source": [
        "# Word Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "nXyY5EUkIGAS",
        "outputId": "d70ca138-b64f-42b2-aa3d-8248b7b85b4f"
      },
      "source": [
        "# memory limit may be exceeded. Try deleting some objects before running this next section\n",
        "# or copy this section to a different notebook.\n",
        "!pip install gensim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (2.2.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OtFF5X_IZJ4"
      },
      "source": [
        "from gensim.models.word2vec import Word2Vec\n",
        "import gensim.downloader as api\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6hNrv_O2nPgD",
        "outputId": "44f96edf-13cf-4ccd-94d6-0ebdf0a8faf7"
      },
      "source": [
        "api.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'corpora': {'20-newsgroups': {'checksum': 'c92fd4f6640a86d5ba89eaad818a9891',\n",
              "   'description': 'The notorious collection of approximately 20,000 newsgroup posts, partitioned (nearly) evenly across 20 different newsgroups.',\n",
              "   'fields': {'data': '',\n",
              "    'id': 'original id inferred from folder name',\n",
              "    'set': \"marker of original split (possible values 'train' and 'test')\",\n",
              "    'topic': 'name of topic (20 variant of possible values)'},\n",
              "   'file_name': '20-newsgroups.gz',\n",
              "   'file_size': 14483581,\n",
              "   'license': 'not found',\n",
              "   'num_records': 18846,\n",
              "   'parts': 1,\n",
              "   'read_more': ['http://qwone.com/~jason/20Newsgroups/'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/20-newsgroups/__init__.py',\n",
              "   'record_format': 'dict'},\n",
              "  '__testing_matrix-synopsis': {'checksum': '1767ac93a089b43899d54944b07d9dc5',\n",
              "   'description': '[THIS IS ONLY FOR TESTING] Synopsis of the movie matrix.',\n",
              "   'file_name': '__testing_matrix-synopsis.gz',\n",
              "   'parts': 1,\n",
              "   'read_more': ['http://www.imdb.com/title/tt0133093/plotsummary?ref_=ttpl_pl_syn#synopsis']},\n",
              "  '__testing_multipart-matrix-synopsis': {'checksum-0': 'c8b0c7d8cf562b1b632c262a173ac338',\n",
              "   'checksum-1': '5ff7fc6818e9a5d9bc1cf12c35ed8b96',\n",
              "   'checksum-2': '966db9d274d125beaac7987202076cba',\n",
              "   'description': '[THIS IS ONLY FOR TESTING] Synopsis of the movie matrix.',\n",
              "   'file_name': '__testing_multipart-matrix-synopsis.gz',\n",
              "   'parts': 3,\n",
              "   'read_more': ['http://www.imdb.com/title/tt0133093/plotsummary?ref_=ttpl_pl_syn#synopsis']},\n",
              "  'fake-news': {'checksum': '5e64e942df13219465927f92dcefd5fe',\n",
              "   'description': \"News dataset, contains text and metadata from 244 websites and represents 12,999 posts in total from a specific window of 30 days. The data was pulled using the webhose.io API, and because it's coming from their crawler, not all websites identified by their BS Detector are present in this dataset. Data sources that were missing a label were simply assigned a label of 'bs'. There are (ostensibly) no genuine, reliable, or trustworthy news sources represented in this dataset (so far), so don't trust anything you read.\",\n",
              "   'fields': {'author': 'author of story',\n",
              "    'comments': 'number of Facebook comments',\n",
              "    'country': 'data from webhose.io',\n",
              "    'crawled': 'date the story was archived',\n",
              "    'domain_rank': 'data from webhose.io',\n",
              "    'language': 'data from webhose.io',\n",
              "    'likes': 'number of Facebook likes',\n",
              "    'main_img_url': 'image from story',\n",
              "    'ord_in_thread': '',\n",
              "    'participants_count': 'number of participants',\n",
              "    'published': 'date published',\n",
              "    'replies_count': 'number of replies',\n",
              "    'shares': 'number of Facebook shares',\n",
              "    'site_url': 'site URL from BS detector',\n",
              "    'spam_score': 'data from webhose.io',\n",
              "    'text': 'text of story',\n",
              "    'thread_title': '',\n",
              "    'title': 'title of story',\n",
              "    'type': 'type of website (label from BS detector)',\n",
              "    'uuid': 'unique identifier'},\n",
              "   'file_name': 'fake-news.gz',\n",
              "   'file_size': 20102776,\n",
              "   'license': 'https://creativecommons.org/publicdomain/zero/1.0/',\n",
              "   'num_records': 12999,\n",
              "   'parts': 1,\n",
              "   'read_more': ['https://www.kaggle.com/mrisdal/fake-news'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/fake-news/__init__.py',\n",
              "   'record_format': 'dict'},\n",
              "  'patent-2017': {'checksum-0': '818501f0b9af62d3b88294d86d509f8f',\n",
              "   'checksum-1': '66c05635c1d3c7a19b4a335829d09ffa',\n",
              "   'description': \"Patent Grant Full Text. Contains the full text including tables, sequence data and 'in-line' mathematical expressions of each patent grant issued in 2017.\",\n",
              "   'file_name': 'patent-2017.gz',\n",
              "   'file_size': 3087262469,\n",
              "   'license': 'not found',\n",
              "   'num_records': 353197,\n",
              "   'parts': 2,\n",
              "   'read_more': ['http://patents.reedtech.com/pgrbft.php'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/patent-2017/__init__.py',\n",
              "   'record_format': 'dict'},\n",
              "  'quora-duplicate-questions': {'checksum': 'd7cfa7fbc6e2ec71ab74c495586c6365',\n",
              "   'description': 'Over 400,000 lines of potential question duplicate pairs. Each line contains IDs for each question in the pair, the full text for each question, and a binary value that indicates whether the line contains a duplicate pair or not.',\n",
              "   'fields': {'id': 'the id of a training set question pair',\n",
              "    'is_duplicate': 'the target variable, set to 1 if question1 and question2 have essentially the same meaning, and 0 otherwise',\n",
              "    'qid1': 'unique ids of each question',\n",
              "    'qid2': 'unique ids of each question',\n",
              "    'question1': 'the full text of each question',\n",
              "    'question2': 'the full text of each question'},\n",
              "   'file_name': 'quora-duplicate-questions.gz',\n",
              "   'file_size': 21684784,\n",
              "   'license': 'probably https://www.quora.com/about/tos',\n",
              "   'num_records': 404290,\n",
              "   'parts': 1,\n",
              "   'read_more': ['https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/quora-duplicate-questions/__init__.py',\n",
              "   'record_format': 'dict'},\n",
              "  'semeval-2016-2017-task3-subtaskA-unannotated': {'checksum': '2de0e2f2c4f91c66ae4fcf58d50ba816',\n",
              "   'description': 'SemEval 2016 / 2017 Task 3 Subtask A unannotated dataset contains 189,941 questions and 1,894,456 comments in English collected from the Community Question Answering (CQA) web forum of Qatar Living. These can be used as a corpus for language modelling.',\n",
              "   'fields': {'RelComments': [{'RELC_DATE': 'date of posting',\n",
              "      'RELC_ID': 'comment identifier',\n",
              "      'RELC_USERID': 'identifier of the user posting the comment',\n",
              "      'RELC_USERNAME': 'name of the user posting the comment',\n",
              "      'RelCText': 'text of answer'}],\n",
              "    'RelQuestion': {'RELQ_CATEGORY': 'question category, according to the Qatar Living taxonomy',\n",
              "     'RELQ_DATE': 'date of posting',\n",
              "     'RELQ_ID': 'question indentifier',\n",
              "     'RELQ_USERID': 'identifier of the user asking the question',\n",
              "     'RELQ_USERNAME': 'name of the user asking the question',\n",
              "     'RelQBody': 'body of question',\n",
              "     'RelQSubject': 'subject of question'},\n",
              "    'THREAD_SEQUENCE': ''},\n",
              "   'file_name': 'semeval-2016-2017-task3-subtaskA-unannotated.gz',\n",
              "   'file_size': 234373151,\n",
              "   'license': 'These datasets are free for general research use.',\n",
              "   'num_records': 189941,\n",
              "   'parts': 1,\n",
              "   'read_more': ['http://alt.qcri.org/semeval2016/task3/',\n",
              "    'http://alt.qcri.org/semeval2016/task3/data/uploads/semeval2016-task3-report.pdf',\n",
              "    'https://github.com/RaRe-Technologies/gensim-data/issues/18',\n",
              "    'https://github.com/Witiko/semeval-2016_2017-task3-subtaskA-unannotated-english'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/semeval-2016-2017-task3-subtaskA-unannotated-eng/__init__.py',\n",
              "   'record_format': 'dict'},\n",
              "  'semeval-2016-2017-task3-subtaskBC': {'checksum': '701ea67acd82e75f95e1d8e62fb0ad29',\n",
              "   'description': 'SemEval 2016 / 2017 Task 3 Subtask B and C datasets contain train+development (317 original questions, 3,169 related questions, and 31,690 comments), and test datasets in English. The description of the tasks and the collected data is given in sections 3 and 4.1 of the task paper http://alt.qcri.org/semeval2016/task3/data/uploads/semeval2016-task3-report.pdf linked in section â€œPapersâ€ of https://github.com/RaRe-Technologies/gensim-data/issues/18.',\n",
              "   'fields': {'2016-dev': ['...'],\n",
              "    '2016-test': ['...'],\n",
              "    '2016-train': ['...'],\n",
              "    '2017-test': ['...']},\n",
              "   'file_name': 'semeval-2016-2017-task3-subtaskBC.gz',\n",
              "   'file_size': 6344358,\n",
              "   'license': 'All files released for the task are free for general research use',\n",
              "   'num_records': -1,\n",
              "   'parts': 1,\n",
              "   'read_more': ['http://alt.qcri.org/semeval2017/task3/',\n",
              "    'http://alt.qcri.org/semeval2017/task3/data/uploads/semeval2017-task3.pdf',\n",
              "    'https://github.com/RaRe-Technologies/gensim-data/issues/18',\n",
              "    'https://github.com/Witiko/semeval-2016_2017-task3-subtaskB-english'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/semeval-2016-2017-task3-subtaskB-eng/__init__.py',\n",
              "   'record_format': 'dict'},\n",
              "  'text8': {'checksum': '68799af40b6bda07dfa47a32612e5364',\n",
              "   'description': 'First 100,000,000 bytes of plain text from Wikipedia. Used for testing purposes; see wiki-english-* for proper full Wikipedia datasets.',\n",
              "   'file_name': 'text8.gz',\n",
              "   'file_size': 33182058,\n",
              "   'license': 'not found',\n",
              "   'num_records': 1701,\n",
              "   'parts': 1,\n",
              "   'read_more': ['http://mattmahoney.net/dc/textdata.html'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/text8/__init__.py',\n",
              "   'record_format': 'list of str (tokens)'},\n",
              "  'wiki-english-20171001': {'checksum-0': 'a7d7d7fd41ea7e2d7fa32ec1bb640d71',\n",
              "   'checksum-1': 'b2683e3356ffbca3b6c2dca6e9801f9f',\n",
              "   'checksum-2': 'c5cde2a9ae77b3c4ebce804f6df542c2',\n",
              "   'checksum-3': '00b71144ed5e3aeeb885de84f7452b81',\n",
              "   'description': 'Extracted Wikipedia dump from October 2017. Produced by `python -m gensim.scripts.segment_wiki -f enwiki-20171001-pages-articles.xml.bz2 -o wiki-en.gz`',\n",
              "   'fields': {'section_texts': 'list of body of sections',\n",
              "    'section_titles': 'list of titles of sections',\n",
              "    'title': 'Title of wiki article'},\n",
              "   'file_name': 'wiki-english-20171001.gz',\n",
              "   'file_size': 6516051717,\n",
              "   'license': 'https://dumps.wikimedia.org/legal.html',\n",
              "   'num_records': 4924894,\n",
              "   'parts': 4,\n",
              "   'read_more': ['https://dumps.wikimedia.org/enwiki/20171001/'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/wiki-english-20171001/__init__.py',\n",
              "   'record_format': 'dict'}},\n",
              " 'models': {'__testing_word2vec-matrix-synopsis': {'checksum': '534dcb8b56a360977a269b7bfc62d124',\n",
              "   'description': '[THIS IS ONLY FOR TESTING] Word vecrors of the movie matrix.',\n",
              "   'file_name': '__testing_word2vec-matrix-synopsis.gz',\n",
              "   'parameters': {'dimensions': 50},\n",
              "   'parts': 1,\n",
              "   'preprocessing': 'Converted to w2v using a preprocessed corpus. Converted to w2v format with `python3.5 -m gensim.models.word2vec -train <input_filename> -iter 50 -output <output_filename>`.',\n",
              "   'read_more': []},\n",
              "  'conceptnet-numberbatch-17-06-300': {'base_dataset': 'ConceptNet, word2vec, GloVe, and OpenSubtitles 2016',\n",
              "   'checksum': 'fd642d457adcd0ea94da0cd21b150847',\n",
              "   'description': 'ConceptNet Numberbatch consists of state-of-the-art semantic vectors (also known as word embeddings) that can be used directly as a representation of word meanings or as a starting point for further machine learning. ConceptNet Numberbatch is part of the ConceptNet open data project. ConceptNet provides lots of ways to compute with word meanings, one of which is word embeddings. ConceptNet Numberbatch is a snapshot of just the word embeddings. It is built using an ensemble that combines data from ConceptNet, word2vec, GloVe, and OpenSubtitles 2016, using a variation on retrofitting.',\n",
              "   'file_name': 'conceptnet-numberbatch-17-06-300.gz',\n",
              "   'file_size': 1225497562,\n",
              "   'license': 'https://github.com/commonsense/conceptnet-numberbatch/blob/master/LICENSE.txt',\n",
              "   'num_records': 1917247,\n",
              "   'parameters': {'dimension': 300},\n",
              "   'parts': 1,\n",
              "   'read_more': ['http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14972',\n",
              "    'https://github.com/commonsense/conceptnet-numberbatch',\n",
              "    'http://conceptnet.io/'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/conceptnet-numberbatch-17-06-300/__init__.py'},\n",
              "  'fasttext-wiki-news-subwords-300': {'base_dataset': 'Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens)',\n",
              "   'checksum': 'de2bb3a20c46ce65c9c131e1ad9a77af',\n",
              "   'description': '1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens).',\n",
              "   'file_name': 'fasttext-wiki-news-subwords-300.gz',\n",
              "   'file_size': 1005007116,\n",
              "   'license': 'https://creativecommons.org/licenses/by-sa/3.0/',\n",
              "   'num_records': 999999,\n",
              "   'parameters': {'dimension': 300},\n",
              "   'parts': 1,\n",
              "   'read_more': ['https://fasttext.cc/docs/en/english-vectors.html',\n",
              "    'https://arxiv.org/abs/1712.09405',\n",
              "    'https://arxiv.org/abs/1607.01759'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/fasttext-wiki-news-subwords-300/__init__.py'},\n",
              "  'glove-twitter-100': {'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)',\n",
              "   'checksum': 'b04f7bed38756d64cf55b58ce7e97b15',\n",
              "   'description': 'Pre-trained vectors based on  2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)',\n",
              "   'file_name': 'glove-twitter-100.gz',\n",
              "   'file_size': 405932991,\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'num_records': 1193514,\n",
              "   'parameters': {'dimension': 100},\n",
              "   'parts': 1,\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-100.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-100/__init__.py'},\n",
              "  'glove-twitter-200': {'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)',\n",
              "   'checksum': 'e52e8392d1860b95d5308a525817d8f9',\n",
              "   'description': 'Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
              "   'file_name': 'glove-twitter-200.gz',\n",
              "   'file_size': 795373100,\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'num_records': 1193514,\n",
              "   'parameters': {'dimension': 200},\n",
              "   'parts': 1,\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-200.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-200/__init__.py'},\n",
              "  'glove-twitter-25': {'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)',\n",
              "   'checksum': '50db0211d7e7a2dcd362c6b774762793',\n",
              "   'description': 'Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
              "   'file_name': 'glove-twitter-25.gz',\n",
              "   'file_size': 109885004,\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'num_records': 1193514,\n",
              "   'parameters': {'dimension': 25},\n",
              "   'parts': 1,\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-25.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-25/__init__.py'},\n",
              "  'glove-twitter-50': {'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)',\n",
              "   'checksum': 'c168f18641f8c8a00fe30984c4799b2b',\n",
              "   'description': 'Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)',\n",
              "   'file_name': 'glove-twitter-50.gz',\n",
              "   'file_size': 209216938,\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'num_records': 1193514,\n",
              "   'parameters': {'dimension': 50},\n",
              "   'parts': 1,\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-50.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-50/__init__.py'},\n",
              "  'glove-wiki-gigaword-100': {'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
              "   'checksum': '40ec481866001177b8cd4cb0df92924f',\n",
              "   'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
              "   'file_name': 'glove-wiki-gigaword-100.gz',\n",
              "   'file_size': 134300434,\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'num_records': 400000,\n",
              "   'parameters': {'dimension': 100},\n",
              "   'parts': 1,\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-100.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-100/__init__.py'},\n",
              "  'glove-wiki-gigaword-200': {'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
              "   'checksum': '59652db361b7a87ee73834a6c391dfc1',\n",
              "   'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
              "   'file_name': 'glove-wiki-gigaword-200.gz',\n",
              "   'file_size': 264336934,\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'num_records': 400000,\n",
              "   'parameters': {'dimension': 200},\n",
              "   'parts': 1,\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-200.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-200/__init__.py'},\n",
              "  'glove-wiki-gigaword-300': {'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
              "   'checksum': '29e9329ac2241937d55b852e8284e89b',\n",
              "   'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
              "   'file_name': 'glove-wiki-gigaword-300.gz',\n",
              "   'file_size': 394362229,\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'num_records': 400000,\n",
              "   'parameters': {'dimension': 300},\n",
              "   'parts': 1,\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-300.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-300/__init__.py'},\n",
              "  'glove-wiki-gigaword-50': {'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
              "   'checksum': 'c289bc5d7f2f02c6dc9f2f9b67641813',\n",
              "   'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
              "   'file_name': 'glove-wiki-gigaword-50.gz',\n",
              "   'file_size': 69182535,\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'num_records': 400000,\n",
              "   'parameters': {'dimension': 50},\n",
              "   'parts': 1,\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-50.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-50/__init__.py'},\n",
              "  'word2vec-google-news-300': {'base_dataset': 'Google News (about 100 billion words)',\n",
              "   'checksum': 'a5e5354d40acb95f9ec66d5977d140ef',\n",
              "   'description': \"Pre-trained vectors trained on a part of the Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. The phrases were obtained using a simple data-driven approach described in 'Distributed Representations of Words and Phrases and their Compositionality' (https://code.google.com/archive/p/word2vec/).\",\n",
              "   'file_name': 'word2vec-google-news-300.gz',\n",
              "   'file_size': 1743563840,\n",
              "   'license': 'not found',\n",
              "   'num_records': 3000000,\n",
              "   'parameters': {'dimension': 300},\n",
              "   'parts': 1,\n",
              "   'read_more': ['https://code.google.com/archive/p/word2vec/',\n",
              "    'https://arxiv.org/abs/1301.3781',\n",
              "    'https://arxiv.org/abs/1310.4546',\n",
              "    'https://www.microsoft.com/en-us/research/publication/linguistic-regularities-in-continuous-space-word-representations/?from=http%3A%2F%2Fresearch.microsoft.com%2Fpubs%2F189726%2Frvecs.pdf'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-google-news-300/__init__.py'},\n",
              "  'word2vec-ruscorpora-300': {'base_dataset': 'Russian National Corpus (about 250M words)',\n",
              "   'checksum': '9bdebdc8ae6d17d20839dd9b5af10bc4',\n",
              "   'description': 'Word2vec Continuous Skipgram vectors trained on full Russian National Corpus (about 250M words). The model contains 185K words.',\n",
              "   'file_name': 'word2vec-ruscorpora-300.gz',\n",
              "   'file_size': 208427381,\n",
              "   'license': 'https://creativecommons.org/licenses/by/4.0/deed.en',\n",
              "   'num_records': 184973,\n",
              "   'parameters': {'dimension': 300, 'window_size': 10},\n",
              "   'parts': 1,\n",
              "   'preprocessing': 'The corpus was lemmatized and tagged with Universal PoS',\n",
              "   'read_more': ['https://www.academia.edu/24306935/WebVectors_a_Toolkit_for_Building_Web_Interfaces_for_Vector_Semantic_Models',\n",
              "    'http://rusvectores.org/en/',\n",
              "    'https://github.com/RaRe-Technologies/gensim-data/issues/3'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-ruscorpora-300/__init__.py'}}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "eytG4hu4nPSB",
        "outputId": "0434b511-bc69-4f06-8486-e5cddcd6363f"
      },
      "source": [
        "model_w2v = api.load(\"word2vec-google-news-300\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:252: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "C0lxhBMeIYrl",
        "outputId": "9c20cd10-86aa-434e-88c2-93fb4633e4e4"
      },
      "source": [
        "model_w2v.most_similar(\"cookies\",topn=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('cookie', 0.745154082775116),\n",
              " ('oatmeal_raisin_cookies', 0.6887780427932739),\n",
              " ('oatmeal_cookies', 0.662139892578125),\n",
              " ('cookie_dough_ice_cream', 0.6520504951477051),\n",
              " ('brownies', 0.6479344964027405),\n",
              " ('homemade_cookies', 0.6476464867591858),\n",
              " ('gingerbread_cookies', 0.6461867690086365),\n",
              " ('Cookies', 0.6341644525527954),\n",
              " ('cookies_cupcakes', 0.6275068521499634),\n",
              " ('cupcakes', 0.6258294582366943)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "rkfcI1niIYQ4",
        "outputId": "7d074131-7042-4c05-8bb7-1031e54a8e23"
      },
      "source": [
        "model_w2v.doesnt_match([\"USA\",\"Canada\",\"India\",\"Tokyo\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tokyo'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "iZ0lCjnaImWZ",
        "outputId": "da50df0e-2366-4d4b-9d6a-82f5c7581f4e"
      },
      "source": [
        "king = model_w2v['king']\n",
        "man = model_w2v['man']\n",
        "woman = model_w2v['woman']\n",
        "\n",
        "queen = king - man + woman  \n",
        "model_w2v.similar_by_vector(queen)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('king', 0.8449392318725586),\n",
              " ('queen', 0.7300517559051514),\n",
              " ('monarch', 0.6454660892486572),\n",
              " ('princess', 0.6156251430511475),\n",
              " ('crown_prince', 0.5818676948547363),\n",
              " ('prince', 0.5777117609977722),\n",
              " ('kings', 0.5613663792610168),\n",
              " ('sultan', 0.5376776456832886),\n",
              " ('Queen_Consort', 0.5344247817993164),\n",
              " ('queens', 0.5289887189865112)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2SSPqujo4K8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}