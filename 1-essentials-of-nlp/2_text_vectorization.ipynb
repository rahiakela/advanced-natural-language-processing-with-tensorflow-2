{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "2-text-vectorization.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "RS6j_Uu1aKr7",
        "Q5NI7lL_aPrR",
        "50W5rWfeOoR3"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/advanced-natural-language-processing-with-tensorflow-2/blob/main/1-essentials-of-nlp/2_text_vectorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdA7sBdGl2S2"
      },
      "source": [
        "## Vectorizing text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Xk0ZWFhsJg7"
      },
      "source": [
        "To understand how to process text, it is important to understand the general\r\n",
        "workflow for NLP.\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/advanced-nlp-with-tensorflow-2/text-processing-workflow.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "The first two steps of the process in the preceding diagram involve collecting labeled data. A supervised model or even a semi-supervised model needs data to operate.\r\n",
        "\r\n",
        "The next step is usually normalizing and featurizing the data. Models have a hard time processing text data as is. There is a lot of hidden structure in a given text that needs to be processed and exposed. These two steps focus on that. \r\n",
        "\r\n",
        "There are a couple of challenges in using the text content of messages. **The first is that text can be of arbitrary lengths.**\r\n",
        "\r\n",
        "Comparing this to image data, we know that each image has a fixed width and height. Even if the corpus of images has a mixture of sizes, images\r\n",
        "can be resized to a common size with minimal loss of information by using a variety of compression mechanisms.\r\n",
        "\r\n",
        "In NLP, this is a bigger problem compared to computer vision. A common approach to handle this is to truncate the text.\r\n",
        "\r\n",
        "**The second issue is that of the representation of words with a numerical quantity or feature.**\r\n",
        "\r\n",
        "In computer vision, the smallest unit is a pixel. Each pixel has a set of\r\n",
        "numerical values indicating color or intensity. \r\n",
        "\r\n",
        "In a text, the smallest unit could be a word. Aggregating the Unicode values of the characters does not convey or embody the meaning of the word.\r\n",
        "\r\n",
        "**A core problem then is to construct a numerical representation of words. Vectorization is the process of converting a word to a vector of numbers that embodies the information contained in the word. Depending on the vectorization technique, this vector may have additional properties that may allow comparison with other words.**\r\n",
        "\r\n",
        "These are the followings text vectorization approach:\r\n",
        "\r\n",
        "- **Count-based vectorization**: The simplest approach for vectorizing is to use counts of words.\r\n",
        "- **TF-IDF based text vectorization**: This is more sophisticated, with its origins in information retrieval.\r\n",
        "- **Word2Vec based text vectorization**: it generate embeddings or word vectors.\r\n",
        "- **BERT based text vectorization**: The newest method in this area.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZPInUZktiyI"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zy9eSdb1cn4Z"
      },
      "source": [
        "%tensorflow_version 2.x     # magic command instructing to use TensorFlow version 2+\n",
        "import tensorflow as tf\n",
        "\n",
        "import os\n",
        "import io\n",
        "import re\n",
        "\n",
        "import pandas as pd \n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "import gensim.downloader as api\n",
        "\n",
        "from keras.utils import np_utils\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOWZnCUXrhlM"
      },
      "source": [
        "# Basic 1-layer neural network model for evaluation\r\n",
        "def make_model(input_dims=3, num_units=12):\r\n",
        "  model = tf.keras.Sequential()\r\n",
        "\r\n",
        "  # Adds a densely-connected layer with 12 units to the model:\r\n",
        "  model.add(tf.keras.layers.Dense(num_units, \r\n",
        "                                  input_dim=input_dims, \r\n",
        "                                  activation='relu'))\r\n",
        "\r\n",
        "  # Add a sigmoid layer with a binary output unit:\r\n",
        "  model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\r\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "  \r\n",
        "  return model"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ3vRSqLvTu6"
      },
      "source": [
        "## Data collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wr1vzM27oyLD"
      },
      "source": [
        "**The first step of any Machine Learning (ML) project is to obtain a dataset.**\r\n",
        "\r\n",
        "We will be using the SMS Spam Collection dataset made available by University of California, Irvine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JSjikP_c0Ba"
      },
      "source": [
        "# Download the zip file\n",
        "path_to_zip = tf.keras.utils.get_file(\"smsspamcollection.zip\",\n",
        "                  origin=\"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\",\n",
        "                  extract=True)\n",
        "\n",
        "# Unzip the file into a folder\n",
        "!unzip $path_to_zip -d data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytFnxCGFaJ20"
      },
      "source": [
        "# optional step - helps if colab gets disconnected\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbXeir-dm0nr"
      },
      "source": [
        "Reading the data file is trivial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Kb8AF3wHaKE3",
        "outputId": "9f4ebd29-62b9-48d3-8560-f26b75062023"
      },
      "source": [
        "# Let's see if we read the data correctly\n",
        "# lines = io.open('/content/drive/My Drive/colab-data/SMSSpamCollection').read().strip().split('\\n')\n",
        "lines = io.open('/content/data/SMSSpamCollection').read().strip().split('\\n')\n",
        "lines[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ham\\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "dYG5LFyDm-C-",
        "outputId": "cd454294-464e-4bd4-a53d-8a381ad01e8d"
      },
      "source": [
        "lines[2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"spam\\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCFBMGdWvnNn"
      },
      "source": [
        "### Pre-process Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zle10-0pnUjv"
      },
      "source": [
        "The next step is to split each line into two columns – one with the text of the message and the other as the label. While we are separating these labels, we will also convert the labels to numeric values. Since we are interested in predicting spam messages, we can assign a value of 1 to the spam\r\n",
        "messages. A value of 0 will be assigned to legitimate messages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhpfi9lWC5We",
        "outputId": "585ab839-1353-4822-d447-5bec6edfd34b"
      },
      "source": [
        "spam_dataset = []\n",
        "spam_count = 0\n",
        "ham_count = 0\n",
        "for line in lines:\n",
        "  label, text = line.split('\\t')\n",
        "  if label.lower().strip() == 'spam':\n",
        "    spam_dataset.append((1, text.strip()))\n",
        "    spam_count += 1\n",
        "  else:\n",
        "    spam_dataset.append(((0, text.strip())))\n",
        "    ham_count += 1\n",
        "\n",
        "spam_dataset[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'),\n",
              " (0, 'Ok lar... Joking wif u oni...'),\n",
              " (1,\n",
              "  \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\"),\n",
              " (0, 'U dun say so early hor... U c already then say...'),\n",
              " (0, \"Nah I don't think he goes to usf, he lives around here though\")]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aV1uWIEUoLWQ",
        "outputId": "a525bbd4-7960-4db1-e251-8baa523cdcbd"
      },
      "source": [
        "print(\"Spam: \", spam_count, \", Ham: \", ham_count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spam:  747 , Ham:  4827\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JReDkDrhoSWG"
      },
      "source": [
        "Now the dataset is ready for further processing in the pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50W5rWfeOoR3"
      },
      "source": [
        "## Count-based vectorization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGPl4OiyoRzY"
      },
      "source": [
        "The idea behind count-based vectorization is really simple. Each unique word\r\n",
        "appearing in the corpus is assigned a column in the vocabulary. Each document,\r\n",
        "which would correspond to individual messages in the spam example, is assigned\r\n",
        "a row. The counts of the words appearing in that document are entered in\r\n",
        "the relevant cell corresponding to the document and the word. \r\n",
        "\r\n",
        "**With $n$ unique documents containing $m$ unique words, this results in a matrix of $n$ rows by $m$ columns.**\r\n",
        "\r\n",
        "Consider a corpus like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cco-INbyhouu"
      },
      "source": [
        "corpus = [\n",
        "  \"I like fruits. Fruits like bananas\",\n",
        "  \"I love bananas but eat an apple\",\n",
        "  \"An apple a day keeps the doctor away\"\n",
        "]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FoicaJMrMwV"
      },
      "source": [
        "There are three documents in this corpus of text. The scikit-learn (sklearn)\r\n",
        "library provides methods for undertaking count-based vectorization.\r\n",
        "\r\n",
        "The `CountVectorizer` class provides a built-in tokenizer that separates the tokens of two or more characters in length. This class takes a variety of options including a custom tokenizer, a stop word list, the option to convert characters to lowercase prior to tokenization, and a binary mode that converts every positive count to 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clVOFDb1ERTL",
        "outputId": "a78d5ca2-9590-46bf-b418-f04959df699a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "vectorizer.get_feature_names()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['an',\n",
              " 'apple',\n",
              " 'away',\n",
              " 'bananas',\n",
              " 'but',\n",
              " 'day',\n",
              " 'doctor',\n",
              " 'eat',\n",
              " 'fruits',\n",
              " 'keeps',\n",
              " 'like',\n",
              " 'love',\n",
              " 'the']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3B3znbFis2qw"
      },
      "source": [
        "The full matrix can be seen as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM7jlBZdFmiD",
        "outputId": "60f07754-acb5-4528-929e-8388c7388905",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X.toarray()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 2, 0, 0],\n",
              "       [1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0],\n",
              "       [1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHrQlO9itGcl"
      },
      "source": [
        "This process has now converted a sentence such as \"I like fruits. Fruits like bananas\" into a vector `(0, 0, 0, 1, 0, 0, 0, 2, 0, 2, 0, 0)`.\r\n",
        "\r\n",
        "**This is an example of context free vectorization. Context-free refers to the fact that the order of the words in the document did not make any difference in the generation of the vector. This is merely counting the instances of the words in a document.**\r\n",
        "\r\n",
        "Consequently, words with multiple meanings may be grouped into one, for example, bank. This may refer to a place near the river or a place to keep money. \r\n",
        "\r\n",
        "**However, it does provide a method to compare documents and derive similarity. The cosine similarity or distance can be computed between two documents, to see which documents are similar to which other documents**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pbT9sH-GwvM",
        "outputId": "4b2182a1-1227-4d48-9192-270d0308325d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cosine_similarity(X.toarray())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.        , 0.13608276, 0.        ],\n",
              "       [0.13608276, 1.        , 0.3086067 ],\n",
              "       [0.        , 0.3086067 , 1.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q61lNJt0t_6v"
      },
      "source": [
        "This shows that the first sentence and the second sentence have a `0.136` similarity score (on a scale of 0 to 1). The first and third sentence have nothing in common. The second and third sentence have a similarity score of `0.308` – the highest in this set.\r\n",
        "\r\n",
        "**Another use case of this technique is to check the similarity of the documents\r\n",
        "with given keywords.**\r\n",
        "\r\n",
        "Let's say that the query is apple and bananas. This first step is\r\n",
        "to compute the vector of this query, and then compute the cosine similarity scores against the documents in the corpus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fe6qPCQ4Gxb0",
        "outputId": "974a3ae0-155b-46a5-efe6-42093f97b628",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "query = vectorizer.transform([\"apple and bananas\"])\n",
        "\n",
        "cosine_similarity(X, query)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.23570226],\n",
              "       [0.57735027],\n",
              "       [0.26726124]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2A93J9gusmf"
      },
      "source": [
        "This shows that this query matches the second sentence in the corpus the best. The third sentence would rank second, and the first sentence would rank lowest.\r\n",
        "\r\n",
        "In a few lines, a basic search engine has been implemented, along with logic to serve queries!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7w2zVnOnER_3"
      },
      "source": [
        "## TF-IDF Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKyNZgATHmhM"
      },
      "source": [
        "transformer = TfidfTransformer(smooth_idf=False)\n",
        "tfidf = transformer.fit_transform(X.toarray())\n",
        "\n",
        "pd.DataFrame(tfidf.toarray(), \n",
        "             columns=vectorizer.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rifoynAZvO9H"
      },
      "source": [
        "tfidf = TfidfVectorizer(binary=True)\n",
        "X = tfidf.fit_transform(train['Message']).astype('float32')\n",
        "X_test = tfidf.transform(test['Message']).astype('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDnG2OPHwD7U"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGujuysLwKJe"
      },
      "source": [
        "from keras.utils import np_utils\n",
        "\n",
        "_, cols = X.shape\n",
        "model2 = make_model(cols)  # to match tf-idf dimensions\n",
        "lb = LabelEncoder()\n",
        "y = lb.fit_transform(y_train)\n",
        "dummy_y_train = np_utils.to_categorical(y)\n",
        "model2.fit(X.toarray(), y_train, epochs=10, batch_size=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXYe6f8my21Y"
      },
      "source": [
        "model2.evaluate(X_test.toarray(), y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptf40YDjh71c"
      },
      "source": [
        "train.loc[train.Spam == 1].describe() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMMB-9H3IEwm"
      },
      "source": [
        "# Word Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXyY5EUkIGAS"
      },
      "source": [
        "# memory limit may be exceeded. Try deleting some objects before running this next section\n",
        "# or copy this section to a different notebook.\n",
        "!pip install gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OtFF5X_IZJ4"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hNrv_O2nPgD"
      },
      "source": [
        "api.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eytG4hu4nPSB"
      },
      "source": [
        "model_w2v = api.load(\"word2vec-google-news-300\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0lxhBMeIYrl"
      },
      "source": [
        "model_w2v.most_similar(\"cookies\",topn=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkfcI1niIYQ4"
      },
      "source": [
        "model_w2v.doesnt_match([\"USA\",\"Canada\",\"India\",\"Tokyo\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ0lCjnaImWZ"
      },
      "source": [
        "king = model_w2v['king']\n",
        "man = model_w2v['man']\n",
        "woman = model_w2v['woman']\n",
        "\n",
        "queen = king - man + woman  \n",
        "model_w2v.similar_by_vector(queen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2SSPqujo4K8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}